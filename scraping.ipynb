{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "    def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "                        express_df[\"news_channel\"].append(\"Express News\")  # Optional\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    def get_dunya_articles(self, max_pages=7):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.dunyanews.tv'\n",
    "        categories = ['Entertainment', 'Pakistan', 'World', 'Sports', 'Business']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/index.php/ur/{category}?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                news_boxes = soup.find_all(\"div\", class_=\"cNewsBox\")\n",
    "                print(f\"\\t--> Found {len(news_boxes)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for news in news_boxes:\n",
    "                    try:\n",
    "                        title_tag = news.find(\"h3\")\n",
    "                        if title_tag:\n",
    "                            link_tag = title_tag.find(\"a\")\n",
    "                            if link_tag:\n",
    "                                title = link_tag.get_text(strip=True)\n",
    "                                link = base_url + link_tag['href']\n",
    "                            else:\n",
    "                                print(\"\\t--> Skipping article due to missing link.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title tag.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        content = \"\"\n",
    "                        content_div = content_soup.find(\"div\", class_=\"main-news\") \n",
    "                        if content_div:\n",
    "                            content_paras = content_div.find_all(\"p\")\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                                for p in content_paras if p.get_text(strip=True)\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        dunya_df['id'].append(self.id)\n",
    "                        dunya_df['title'].append(title)\n",
    "                        dunya_df['link'].append(link)\n",
    "                        dunya_df['gold_label'].append(category)\n",
    "                        dunya_df['content'].append(content)\n",
    "                        dunya_df[\"news_channel\"].append(\"Dunya News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(dunya_df)\n",
    "\n",
    "    def get_geo_articles(self, max_pages=7):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv/category'\n",
    "        categories = ['business', 'entertainment', 'sports', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/page/{page}/\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 403:\n",
    "                    print(\"Request was blocked by the server.\")\n",
    "                    break\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find_all(\"div\", class_=\"m_pic\")\n",
    "\n",
    "                print(f\"\\t--> Found {len(articles)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        title_tag = article.find(\"a\", class_=\"open-section\")\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get(\"title\", \"\").strip()\n",
    "                            link = title_tag[\"href\"]\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        \n",
    "                        content_div = content_soup.find(\"div\", class_=\"content-area\")\n",
    "                        content = \"\"\n",
    "                        if content_div:\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True)\n",
    "                                for p in content_div.find_all(\"p\")\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        geo_df[\"id\"].append(self.id)\n",
    "                        geo_df[\"title\"].append(title)\n",
    "                        geo_df[\"link\"].append(link)\n",
    "                        geo_df[\"gold_label\"].append(category.capitalize())\n",
    "                        geo_df[\"content\"].append(content)\n",
    "                        geo_df[\"news_channel\"].append(\"Geo News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "\n",
    "    def get_jang_articles(self):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "\n",
    "        # Base URL and categories to scrape\n",
    "        base_url = 'https://jang.com.pk/category/latest-news'\n",
    "        categories = ['entertainment', 'sports', 'world', 'health-science']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find article blocks in the main content section\n",
    "            articles = soup.find('ul',class_='scrollPaginationNew__').find_all(\"li\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    if article.get(\"class\") == [\"ad_latest_stories\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    title, link = None, None\n",
    "\n",
    "                    # Look for 'main-pic' div to extract the link and title\n",
    "                    main_pic = article.find(\"div\", class_=\"main-pic\")\n",
    "                    if main_pic:\n",
    "                        link_tag = main_pic.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.get(\"title\", \"\").strip()\n",
    "                            print(title)\n",
    "\n",
    "                    # Fallback: Check 'main-heading' div for title and link if not found in 'main-pic'\n",
    "                    if not title or not link:\n",
    "                        main_heading = article.find(\"div\", class_=\"main-heading\")\n",
    "                        if main_heading:\n",
    "                            link_tag = main_heading.find(\"a\", href=True)\n",
    "                            if link_tag:\n",
    "                                link = link_tag[\"href\"]\n",
    "                                title_tag = link_tag.find(\"h2\")\n",
    "                                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    # Fetch article content\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    # Locate content inside relevant div and extract text from <p> tags\n",
    "                    content_div = content_soup.find(\"div\", class_=\"detail_view_content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    # Add data to DataFrame\n",
    "                    jang_df[\"id\"].append(self.id)\n",
    "                    jang_df[\"title\"].append(title)\n",
    "                    jang_df[\"link\"].append(link)\n",
    "                    jang_df[\"gold_label\"].append(category.capitalize())\n",
    "                    jang_df[\"content\"].append(content)\n",
    "                    jang_df[\"news_channel\"].append(\"Jang\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(jang_df)\n",
    "\n",
    "    def get_dawn_articles(self):\n",
    "        dawn_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.dawnnews.tv/'\n",
    "        categories = ['business','sport', 'tech', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find article blocks in the main content section\n",
    "            articles = soup.find('div',class_='flex flex-row w-auto').find_all(\"article\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_div = article.find_all(\"h2\", class_=[\"story__title\"])\n",
    "                    main_div = main_div[0] if main_div else None\n",
    "                    if main_div:\n",
    "                        link_tag = main_div.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.text.strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    # Fetch article content\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    # Locate content inside relevant div and extract text from <p> tags\n",
    "                    content_div = content_soup.find_all(\"div\", class_=[\"story__content\"])\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        for div in content_div:\n",
    "                            content += \" \".join(\n",
    "                                p.get_text(strip=True)\n",
    "                                for p in div.find_all(\"p\")\n",
    "                            )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    # Add data to DataFrame\n",
    "                    dawn_df[\"id\"].append(self.id)\n",
    "                    dawn_df[\"title\"].append(title)\n",
    "                    dawn_df[\"link\"].append(link)\n",
    "                    dawn_df[\"gold_label\"].append(category.capitalize())\n",
    "                    dawn_df[\"content\"].append(content)\n",
    "                    dawn_df[\"news_channel\"].append(\"Dawn News\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(dawn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping category 'business'...\n",
      "\t--> Found 56 articles in 'business'.\n",
      "آئندہ 10 سال میں بجلی کی قیمت مزید ناقابل برداشت ہو سکتی ہے، وفاقی وزیر توانائی\n",
      "اسٹیٹ بینک کا شرح سود 2.5 فیصد کم کرکے 15 فیصد کرنے کا اعلان\n",
      "بلی تھیلے سے باہر آگئی، شریف فیملی پی آئی اے کو اونے پونے داموں خریدنا چاہتی ہے، علی امین گنڈاپور\n",
      "گرے مارکیٹ کے دوبارہ وجود نے سال 2023 کی یادیں تازہ کردیں\n",
      "انٹر بینک مارکیٹ میں ڈالر کے مقابلے میں روپے کی قدر مزید 10 پیسے بہتر ہوئی\n",
      "\t--> Successfully scraped 5 articles from 'business'.\n",
      "Scraping category 'sport'...\n",
      "\t--> Found 63 articles in 'sport'.\n",
      "سری لنکا اے کے خلاف ہوم سیریز کیلئے محمد ہریرہ پاکستان شاہینز کے کپتان مقرر\n",
      "ہدف کا دفاع کرسکتے تھے، بدقسمتی سے میچ جیتنے میں ناکام رہے، حارث رؤف\n",
      "پہلا ون ڈے: آسٹریلیا نے دلچسپ مقابلے کے بعد پاکستان کو شکست دے دی\n",
      "سری لنکا نے پاکستان کو شکست دے کر ہانگ کانگ سپر سکسز کا ٹائٹل اپنے نام کرلیا\n",
      "تنازعات میں اُلجھی پاکستانی ٹیم کا مشکل ترین دورہ آسٹریلیا\n",
      "\t--> Successfully scraped 5 articles from 'sport'.\n",
      "Scraping category 'tech'...\n",
      "\t--> Found 59 articles in 'tech'.\n",
      "انسٹاگرام پر بہت کم دیکھی جانے والی ویڈیوز کی کوالٹی بدتر کیے جانے کا اعتراف\n",
      "واٹس ایپ میسیجز میں گوگل سرچ کی آزمائش\n",
      "کراچی: این ای ڈی یونیورسٹی میں 17 منزلہ آئی ٹی ٹاور بنے گا\n",
      "گوگل پر دنیا کی مجموعی آمدن سے اربوں گنا زیادہ جرمانہ عائد\n",
      "وکی پیڈیا کو بھارت میں قانونی جنگ کا سامنا\n",
      "\t--> Successfully scraped 5 articles from 'tech'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 83 articles in 'world'.\n",
      "امریکا کے صدر کا فیصلہ کرنے والی 7 اہم ریاستیں کونسی ہیں؟\n",
      "ایرانی فورسز کی صوبے سیستان بلوچستان میں کارروائی، 8 عسکریت پسند ہلاک\n",
      "ووٹنگ سے قبل ڈونلڈ ٹرمپ کی پرانی فحش گفتگو ٹک ٹاک پر وائرل\n",
      "امریکی صدارتی انتخاب: 40 سے زائد ریاستوں میں ووٹنگ جاری، ٹرمپ اور کاملہ کے درمیان سخت مقابلہ\n",
      "ابراہیم رئیسی کی وفات: ایران نے ساری قیاس آرائیاں دفن کردیں\n",
      "\t--> Successfully scraped 5 articles from 'world'.\n"
     ]
    }
   ],
   "source": [
    "# express_df = scraper.get_express_articles()\n",
    "# dunya_df = scraper.get_dunya_articles()\n",
    "# geo_df = scraper.get_geo_articles()\n",
    "# jang_df = scraper.get_jang_articles()\n",
    "dawn_df = scraper.get_dawn_articles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>news_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>آئندہ 10 سال میں بجلی کی قیمت مزید ناقابل بردا...</td>\n",
       "      <td>https://www.dawnnews.tv/news/1245692/</td>\n",
       "      <td>وفاقی وزیر توانائی اویس لغاری نے خبردار کرتے ہ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Dawn News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>اسٹیٹ بینک کا شرح سود 2.5 فیصد کم کرکے 15 فیصد...</td>\n",
       "      <td>https://www.dawnnews.tv/news/1245660/</td>\n",
       "      <td>اسٹیٹ بینک نے بڑا فیصلہ کرتے ہوئے  شرح سود 250...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Dawn News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>بلی تھیلے سے باہر آگئی، شریف فیملی پی آئی اے ...</td>\n",
       "      <td>https://www.dawnnews.tv/news/1245654/</td>\n",
       "      <td>وزیراعلیٰ خیبرپختونخوا علی امین گنڈاپور  نے شر...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Dawn News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>گرے مارکیٹ کے دوبارہ وجود نے سال 2023 کی یادیں...</td>\n",
       "      <td>https://www.dawnnews.tv/news/1239450/</td>\n",
       "      <td>ماہرین نے کہا ہے کہ پاکستان میں  غیر قانونی طو...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Dawn News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>انٹر بینک مارکیٹ میں ڈالر کے مقابلے میں روپے ک...</td>\n",
       "      <td>https://www.dawnnews.tv/news/1234223/</td>\n",
       "      <td>انٹر بینک مارکیٹ میں امریکی ڈالر کے مقابلے میں...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Dawn News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  آئندہ 10 سال میں بجلی کی قیمت مزید ناقابل بردا...   \n",
       "1   1  اسٹیٹ بینک کا شرح سود 2.5 فیصد کم کرکے 15 فیصد...   \n",
       "2   2  بلی تھیلے سے باہر آگئی، شریف فیملی پی آئی اے ...   \n",
       "3   3  گرے مارکیٹ کے دوبارہ وجود نے سال 2023 کی یادیں...   \n",
       "4   4  انٹر بینک مارکیٹ میں ڈالر کے مقابلے میں روپے ک...   \n",
       "\n",
       "                                    link  \\\n",
       "0  https://www.dawnnews.tv/news/1245692/   \n",
       "1  https://www.dawnnews.tv/news/1245660/   \n",
       "2  https://www.dawnnews.tv/news/1245654/   \n",
       "3  https://www.dawnnews.tv/news/1239450/   \n",
       "4  https://www.dawnnews.tv/news/1234223/   \n",
       "\n",
       "                                             content gold_label news_channel  \n",
       "0  وفاقی وزیر توانائی اویس لغاری نے خبردار کرتے ہ...   Business    Dawn News  \n",
       "1  اسٹیٹ بینک نے بڑا فیصلہ کرتے ہوئے  شرح سود 250...   Business    Dawn News  \n",
       "2  وزیراعلیٰ خیبرپختونخوا علی امین گنڈاپور  نے شر...   Business    Dawn News  \n",
       "3  ماہرین نے کہا ہے کہ پاکستان میں  غیر قانونی طو...   Business    Dawn News  \n",
       "4  انٹر بینک مارکیٹ میں امریکی ڈالر کے مقابلے میں...   Business    Dawn News  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# express_df.head()\n",
    "# dunya_df.head()\n",
    "# geo_df.head()\n",
    "# jang_df.head()\n",
    "dawn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b9585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
