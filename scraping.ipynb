{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "    def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "                        express_df[\"news_channel\"].append(\"Express News\")  # Optional\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    def get_dunya_articles(self, max_pages=7):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.dunyanews.tv'\n",
    "        categories = ['Entertainment', 'Pakistan', 'World', 'Sports', 'Business']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/index.php/ur/{category}?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                news_boxes = soup.find_all(\"div\", class_=\"cNewsBox\")\n",
    "                print(f\"\\t--> Found {len(news_boxes)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for news in news_boxes:\n",
    "                    try:\n",
    "                        title_tag = news.find(\"h3\")\n",
    "                        if title_tag:\n",
    "                            link_tag = title_tag.find(\"a\")\n",
    "                            if link_tag:\n",
    "                                title = link_tag.get_text(strip=True)\n",
    "                                link = base_url + link_tag['href']\n",
    "                            else:\n",
    "                                print(\"\\t--> Skipping article due to missing link.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title tag.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        content = \"\"\n",
    "                        content_div = content_soup.find(\"div\", class_=\"main-news\") \n",
    "                        if content_div:\n",
    "                            content_paras = content_div.find_all(\"p\")\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                                for p in content_paras if p.get_text(strip=True)\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        dunya_df['id'].append(self.id)\n",
    "                        dunya_df['title'].append(title)\n",
    "                        dunya_df['link'].append(link)\n",
    "                        dunya_df['gold_label'].append(category)\n",
    "                        dunya_df['content'].append(content)\n",
    "                        dunya_df[\"news_channel\"].append(\"Dunya News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(dunya_df)\n",
    "\n",
    "    def get_geo_articles(self, max_pages=7):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv/category'\n",
    "        categories = ['business', 'entertainment', 'sports', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/page/{page}/\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 403:\n",
    "                    print(\"Request was blocked by the server.\")\n",
    "                    break\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find_all(\"div\", class_=\"m_pic\")\n",
    "\n",
    "                print(f\"\\t--> Found {len(articles)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        title_tag = article.find(\"a\", class_=\"open-section\")\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get(\"title\", \"\").strip()\n",
    "                            link = title_tag[\"href\"]\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        \n",
    "                        content_div = content_soup.find(\"div\", class_=\"content-area\")\n",
    "                        content = \"\"\n",
    "                        if content_div:\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True)\n",
    "                                for p in content_div.find_all(\"p\")\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        geo_df[\"id\"].append(self.id)\n",
    "                        geo_df[\"title\"].append(title)\n",
    "                        geo_df[\"link\"].append(link)\n",
    "                        geo_df[\"gold_label\"].append(category.capitalize())\n",
    "                        geo_df[\"content\"].append(content)\n",
    "                        geo_df[\"news_channel\"].append(\"Geo News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "\n",
    "    def get_jang_articles(self):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "\n",
    "        base_url = 'https://jang.com.pk/category/latest-news'\n",
    "        categories = ['entertainment', 'sports', 'world', 'health-science']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('ul',class_='scrollPaginationNew__').find_all(\"li\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    if article.get(\"class\") == [\"ad_latest_stories\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_pic = article.find(\"div\", class_=\"main-pic\")\n",
    "                    if main_pic:\n",
    "                        link_tag = main_pic.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.get(\"title\", \"\").strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        main_heading = article.find(\"div\", class_=\"main-heading\")\n",
    "                        if main_heading:\n",
    "                            link_tag = main_heading.find(\"a\", href=True)\n",
    "                            if link_tag:\n",
    "                                link = link_tag[\"href\"]\n",
    "                                title_tag = link_tag.find(\"h2\")\n",
    "                                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"detail_view_content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    jang_df[\"id\"].append(self.id)\n",
    "                    jang_df[\"title\"].append(title)\n",
    "                    jang_df[\"link\"].append(link)\n",
    "                    jang_df[\"gold_label\"].append(category.capitalize())\n",
    "                    jang_df[\"content\"].append(content)\n",
    "                    jang_df[\"news_channel\"].append(\"Jang\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(jang_df)\n",
    "\n",
    "    def get_dawn_articles(self):\n",
    "        dawn_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.dawnnews.tv/'\n",
    "        categories = ['business','sport', 'tech', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('div',class_='flex flex-row w-auto').find_all(\"article\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_div = article.find(\"h2\", class_=\"story__title\")\n",
    "                    if main_div:\n",
    "                        link_tag = main_div.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.text.strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    # Fetch article content\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"story__content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    dawn_df[\"id\"].append(self.id)\n",
    "                    dawn_df[\"title\"].append(title)\n",
    "                    dawn_df[\"link\"].append(link)\n",
    "                    dawn_df[\"gold_label\"].append(category.capitalize())\n",
    "                    dawn_df[\"content\"].append(content)\n",
    "                    dawn_df[\"news_channel\"].append(\"Dawn News\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(dawn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'saqafat'.\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "\n",
      "Scraping page 1 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 1 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Entertainment'.\n",
      "Scraping page 2 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 2 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Entertainment'.\n",
      "Scraping page 3 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 3 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Entertainment'.\n",
      "Scraping page 4 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 4 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Entertainment'.\n",
      "Scraping page 5 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 5 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Entertainment'.\n",
      "Scraping page 6 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 6 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Entertainment'.\n",
      "Scraping page 7 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 7 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Entertainment'.\n",
      "\n",
      "Scraping page 1 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 1 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Pakistan'.\n",
      "Scraping page 2 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 2 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Pakistan'.\n",
      "Scraping page 3 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 3 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Pakistan'.\n",
      "Scraping page 4 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 4 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Pakistan'.\n",
      "Scraping page 5 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 5 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Pakistan'.\n",
      "Scraping page 6 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 6 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Pakistan'.\n",
      "Scraping page 7 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 7 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Pakistan'.\n",
      "\n",
      "Scraping page 1 of category 'World'...\n",
      "\t--> Found 18 articles on page 1 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'World'.\n",
      "Scraping page 2 of category 'World'...\n",
      "\t--> Found 18 articles on page 2 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'World'.\n",
      "Scraping page 3 of category 'World'...\n",
      "\t--> Found 18 articles on page 3 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'World'.\n",
      "Scraping page 4 of category 'World'...\n",
      "\t--> Found 18 articles on page 4 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'World'.\n",
      "Scraping page 5 of category 'World'...\n",
      "\t--> Found 18 articles on page 5 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'World'.\n",
      "Scraping page 6 of category 'World'...\n",
      "\t--> Found 18 articles on page 6 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'World'.\n",
      "Scraping page 7 of category 'World'...\n",
      "\t--> Found 18 articles on page 7 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'World'.\n",
      "\n",
      "Scraping page 1 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 1 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Sports'.\n",
      "Scraping page 2 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 2 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Sports'.\n",
      "Scraping page 3 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 3 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Sports'.\n",
      "Scraping page 4 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 4 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Sports'.\n",
      "Scraping page 5 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 5 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Sports'.\n",
      "Scraping page 6 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 6 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Sports'.\n",
      "Scraping page 7 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 7 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Sports'.\n",
      "\n",
      "Scraping page 1 of category 'Business'...\n",
      "\t--> Found 18 articles on page 1 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Business'.\n",
      "Scraping page 2 of category 'Business'...\n",
      "\t--> Found 18 articles on page 2 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Business'.\n",
      "Scraping page 3 of category 'Business'...\n",
      "\t--> Found 18 articles on page 3 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Business'.\n",
      "Scraping page 4 of category 'Business'...\n",
      "\t--> Found 18 articles on page 4 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Business'.\n",
      "Scraping page 5 of category 'Business'...\n",
      "\t--> Found 18 articles on page 5 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Business'.\n",
      "Scraping page 6 of category 'Business'...\n",
      "\t--> Found 18 articles on page 6 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Business'.\n",
      "Scraping page 7 of category 'Business'...\n",
      "\t--> Found 18 articles on page 7 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Business'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 19 articles on page 1 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 19 articles on page 2 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 18 articles on page 3 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 18 articles on page 4 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 18 articles on page 5 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 18 articles on page 6 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 19 articles on page 7 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 7 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 1 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 1 of 'entertainment'.\n",
      "Scraping page 2 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 2 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 2 of 'entertainment'.\n",
      "Scraping page 3 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 3 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 3 of 'entertainment'.\n",
      "Scraping page 4 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 4 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 4 of 'entertainment'.\n",
      "Scraping page 5 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 5 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 5 of 'entertainment'.\n",
      "Scraping page 6 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 6 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 6 of 'entertainment'.\n",
      "Scraping page 7 of category 'entertainment'...\n",
      "\t--> Found 19 articles on page 7 of 'entertainment'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 7 of 'entertainment'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 19 articles on page 1 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 19 articles on page 2 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 19 articles on page 3 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 19 articles on page 4 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 19 articles on page 5 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 19 articles on page 6 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 19 articles on page 7 of 'sports'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 7 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 19 articles on page 1 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 19 articles on page 2 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 19 articles on page 3 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 19 articles on page 4 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 19 articles on page 5 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 19 articles on page 6 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 19 articles on page 7 of 'world'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 7 of 'world'.\n",
      "\n",
      "Scraping category 'entertainment'...\n",
      "\t--> Found 101 articles in 'entertainment'.\n",
      "        \n",
      "         \n",
      "             \n",
      "                \n",
      "       5       \n",
      "62           \n",
      "          \n",
      "    12   \n",
      "        :  \n",
      "         \n",
      "           \n",
      "      \n",
      "          \n",
      "            \n",
      "             \n",
      "        \n",
      "           \n",
      "    :   \n",
      "          \n",
      "          \n",
      "              \n",
      "             \n",
      "         \n",
      "38           \n",
      "          \n",
      "           \n",
      "           \n",
      "            \n",
      "        \n",
      "            \n",
      "              \n",
      "       2    \n",
      "    59   \n",
      "       :  \n",
      "             \n",
      "            \n",
      "           \n",
      "              \n",
      "             \n",
      "                \n",
      "                 \n",
      ":          \n",
      "         :       \n",
      "  :       \n",
      "51             \n",
      "               :  \n",
      "            \n",
      "          \n",
      "     3       \n",
      "  : 36         \n",
      "            \n",
      "   2         \n",
      "         \n",
      "            \n",
      "  :           \n",
      "            :  \n",
      "           \n",
      "          \n",
      "         :  \n",
      "           \n",
      "       \n",
      "            \n",
      "             \n",
      "              \n",
      "             \n",
      "     35     \n",
      "        :  \n",
      "         \n",
      "        \n",
      "    2           \n",
      "          \n",
      "          \n",
      "               \n",
      "   6         \n",
      "       35     \n",
      "           \n",
      "             \n",
      "  :           \n",
      "               \n",
      "          \n",
      "        80     \n",
      "           \n",
      "  : 33         \n",
      "           \n",
      "         \n",
      "          \n",
      "                \n",
      "            \n",
      "  3          \n",
      "            \n",
      "               \n",
      "             \n",
      "             \n",
      "         \n",
      "             \n",
      "  :         \n",
      "  : 31         \n",
      "          \n",
      "      38      \n",
      "\t--> Successfully scraped 99 articles from 'entertainment'.\n",
      "Scraping category 'sports'...\n",
      "\t--> Found 100 articles in 'sports'.\n",
      "   5        \n",
      "          \n",
      "             \n",
      "         \n",
      "              \n",
      "           \n",
      "            \n",
      "               \n",
      "           \n",
      "      \n",
      "   :       \n",
      "            \n",
      "        \n",
      "            \n",
      "   19        \n",
      "         \n",
      "       2    \n",
      "         \n",
      "             \n",
      "           \n",
      "                \n",
      "        \n",
      "           \n",
      "          :  \n",
      "      :   \n",
      "     3        \n",
      "       50  \n",
      "          \n",
      "    :        \n",
      "            \n",
      "          \n",
      "            \n",
      "     19     \n",
      "              \n",
      "            \n",
      "          \n",
      "  20  6      \n",
      "         \n",
      "          :  \n",
      "              \n",
      "   :        \n",
      "   :             \n",
      "        \n",
      "        \n",
      "          \n",
      "           \n",
      "       \n",
      "   :        \n",
      "          \n",
      "          \n",
      "          \n",
      "           \n",
      "            \n",
      "      10     \n",
      "               \n",
      "               \n",
      "     6     2      \n",
      "   1           :  \n",
      "                  \n",
      "        \n",
      "          \n",
      "       \n",
      "        \n",
      "               \n",
      "          \n",
      "      \n",
      "        45  \n",
      "           \n",
      "      \n",
      "       \n",
      "         \n",
      "          \n",
      "           \n",
      "       \n",
      "           \n",
      "          \n",
      "          \n",
      "          \n",
      "        :  \n",
      "   7    \n",
      "          \n",
      "         \n",
      "            \n",
      "           \n",
      "              \n",
      "          \n",
      "        \n",
      "           \n",
      "           \n",
      "        \n",
      "             \n",
      "    :          \n",
      "    7   \n",
      "           \n",
      "       PCB    \n",
      "            \n",
      "   20      \n",
      "              \n",
      "\t--> Successfully scraped 98 articles from 'sports'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 101 articles in 'world'.\n",
      ": 50    25   18   \n",
      "       \n",
      ": 5         \n",
      "          \n",
      "         \n",
      "            \n",
      "             \n",
      "         \n",
      "            \n",
      "  :          \n",
      "     8       \n",
      "          \n",
      "           \n",
      "         \n",
      "         \n",
      "            \n",
      "  :             \n",
      ":               \n",
      "  :        \n",
      "  :          \n",
      "      \n",
      ":                 \n",
      ":          \n",
      "         \n",
      "                \n",
      "           \n",
      "           \n",
      "          17  \n",
      "     6   7  \n",
      "          \n",
      "             \n",
      ":           \n",
      "       \n",
      "               \n",
      "            \n",
      "   95   :   \n",
      "          \n",
      ":       37  \n",
      "         \n",
      ":        \n",
      "     10            \n",
      "         \n",
      "        \n",
      "          \n",
      "         25  \n",
      "     3            \n",
      "           \n",
      ":         36  \n",
      "    29     \n",
      "            5  \n",
      "               \n",
      " :            \n",
      ":                 \n",
      ":       \n",
      "           :  \n",
      "              \n",
      "            \n",
      "               \n",
      ": 48    50      16   \n",
      "   14      \n",
      "               \n",
      ":           \n",
      "        19    \n",
      "         23  \n",
      "          \n",
      "             \n",
      "           \n",
      "          \n",
      "      55  \n",
      "        \n",
      "            \n",
      "    2      \n",
      "           \n",
      "               \n",
      "            \n",
      ":         \n",
      "                  \n",
      "          :    \n",
      "          \n",
      "          \n",
      "     52      \n",
      "       \n",
      "                \n",
      "      \n",
      "        \n",
      "               \n",
      "   778   68   \n",
      "  :        \n",
      "  :             \n",
      "             \n",
      "            \n",
      "             \n",
      "      1   6 \n",
      "             \n",
      "          \n",
      "        \n",
      "                \n",
      "             \n",
      "           \n",
      "\t--> Successfully scraped 99 articles from 'world'.\n",
      "Scraping category 'health-science'...\n",
      "\t--> Found 101 articles in 'health-science'.\n",
      ":    3   2   \n",
      ":          \n",
      "         46  \n",
      ":       10         \n",
      "       99   \n",
      ":    118  \n",
      "       \n",
      ": 7           \n",
      "         \n",
      "51            \n",
      "      141  \n",
      ":     363  \n",
      "        \n",
      "         \n",
      "          \n",
      "        82         \n",
      "          \n",
      "        \n",
      "           \n",
      ":            \n",
      ":     21   \n",
      "           \n",
      "           \n",
      "     \n",
      "        \n",
      "             \n",
      "   150       \n",
      "  :    56  \n",
      "      50      :   \n",
      "  : 33   5        \n",
      "       \n",
      ": 7         \n",
      "               \n",
      "      \n",
      "            \n",
      "  ,      \n",
      ":            \n",
      "        \n",
      "            18  \n",
      ":     1  935    172  \n",
      ":     1647    \n",
      ":    128   1   \n",
      "     \n",
      ":              \n",
      "         :   \n",
      "         :  \n",
      "     40   \n",
      "       \n",
      "       \n",
      "      24   6   \n",
      ":        \n",
      "     38000    :   \n",
      "       20   \n",
      "       \n",
      "           \n",
      ": 19     10   \n",
      "      3   \n",
      "      2        39 \n",
      "        :   \n",
      ":      \n",
      " 9   1        :  \n",
      "               \n",
      ":    112  \n",
      "        \n",
      ": 3      \n",
      "               \n",
      "          \n",
      "        20      \n",
      "             \n",
      "     !\n",
      "        \n",
      ":            \n",
      "            \n",
      "        \n",
      "           \n",
      ":         \n",
      "           500      \n",
      "         \n",
      ":            \n",
      "          \n",
      ":     4     \n",
      ":     114  \n",
      "          \n",
      "            \n",
      ":  149      \n",
      ":    160  \n",
      "        \n",
      ": 10   90     \n",
      " :    10      \n",
      ":        \n",
      "       \n",
      "             \n",
      ":          \n",
      ":         \n",
      "     4    \n",
      "               \n",
      "2024         \n",
      ":  62   \n",
      ":  24      109  \n",
      "\t--> Successfully scraped 99 articles from 'health-science'.\n",
      "Scraping category 'business'...\n",
      "\t--> Found 56 articles in 'business'.\n",
      " 10              \n",
      "     2.5    15    \n",
      "                   \n",
      "       2023    \n",
      "            10   \n",
      "            \n",
      "            \n",
      "           \n",
      "    :     85     \n",
      "                \n",
      "     40      \n",
      " 10              \n",
      "        \n",
      "               \n",
      "                  \n",
      "                   \n",
      "            \n",
      "           \n",
      "                  \n",
      "          \n",
      "            \n",
      "        \n",
      "    2  500    \n",
      "      900    \n",
      "          \n",
      "        \n",
      "  13    10.8     \n",
      "          \n",
      "              \n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245487/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245505/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245488/\n",
      "   3           \n",
      "       7.17  \n",
      "    2  500    \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245456/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245467/\n",
      "24        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245447/\n",
      "       101      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245443/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245427/\n",
      "      3         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245423/\n",
      "    : 85            10     \n",
      "  3    261        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245408/\n",
      "      2.0          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245407/\n",
      "               \n",
      "           \n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "              \n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "\t--> Successfully scraped 36 articles from 'business'.\n",
      "Scraping category 'sport'...\n",
      "\t--> Found 63 articles in 'sport'.\n",
      "   :        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245766/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245765/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245667/\n",
      "  :           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245634/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245310/\n",
      "               \n",
      "-  :           \n",
      "    :       \n",
      "                \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245604/\n",
      " 25       \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245594/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245765/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245667/\n",
      "  :           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245634/\n",
      "                \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245604/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245589/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245584/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "                \n",
      "              \n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245474/\n",
      "-            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245402/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245310/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245329/\n",
      "68             \n",
      "                \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245113/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245133/\n",
      "     20      \n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245107/\n",
      "             20    \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245094/\n",
      "          20     \n",
      "            \n",
      "                  \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245057/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245052/\n",
      " :               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245043/\n",
      "           12      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245004/\n",
      "           20      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245030/\n",
      "           3      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244978/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244996/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244990/\n",
      "               \n",
      "    :        7    \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244957/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244954/\n",
      "         156   \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244936/\n",
      "    :  267      3   73  194    \n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "         \n",
      "\t--> Skipping article '         ' due to missing content.\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Skipping article '        ' due to missing content.\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "\t--> Successfully scraped 11 articles from 'sport'.\n",
      "Scraping category 'tech'...\n",
      "\t--> Found 59 articles in 'tech'.\n",
      "            \n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245677/\n",
      "       \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245545/\n",
      ":      17      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245525/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245471/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245405/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245240/\n",
      "       \n",
      "    16  6      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243708/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243264/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245341/\n",
      "           \n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245286/\n",
      "         16     \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245249/\n",
      "         \n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245159/\n",
      "                \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245011/\n",
      "               \n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244792/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244755/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Skipping article '1947        ' due to missing content.\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244639/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244530/\n",
      "       \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244475/\n",
      "            \n",
      "       \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244409/\n",
      "   3     3      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244255/\n",
      "        200 \n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244060/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243936/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243863/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243860/\n",
      "          \n",
      "    16  6      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243708/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243598/\n",
      "   3       \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243554/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243490/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243432/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243409/\n",
      "             \n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Skipping article '           ' due to missing content.\n",
      "\t--> Successfully scraped 9 articles from 'tech'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 83 articles in 'world'.\n",
      "  :           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245785/\n",
      "   :               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245773/\n",
      "                 \n",
      "       7    \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245757/\n",
      "   :       \n",
      "  :            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1232693/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1232608/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1231675/\n",
      "        8   \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245754/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245747/\n",
      ":              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245759/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245653/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245595/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245415/\n",
      "  :      31        \n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245753/\n",
      ":              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245759/\n",
      "       36  \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245670/\n",
      "           \n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245405/\n",
      "  6  :            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245752/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245728/\n",
      "                \n",
      "                 \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243685/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243216/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1242954/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245712/\n",
      "       36  \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245670/\n",
      "                \n",
      " 8           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1235133/\n",
      " 5               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1235123/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1235057/\n",
      "              \n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245671/\n",
      "  :           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245785/\n",
      "        8   \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245754/\n",
      "          50   84  \n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245664/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245651/\n",
      "       94    \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1244325/\n",
      "   90        42     \n",
      "     150  \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1243436/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1242432/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245653/\n",
      ":             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245615/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "      23   53          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245610/\n",
      "          \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245595/\n",
      "                \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245553/\n",
      "          50   84  \n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245541/\n",
      "                \n",
      "  :        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245485/\n",
      "           \n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "         155 \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245429/\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245419/\n",
      "               \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245415/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245378/\n",
      "       4   \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245397/\n",
      "             \n",
      "               \n",
      "           \n",
      "        82     \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245362/\n",
      "             \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245709/\n",
      "            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245649/\n",
      "              \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245637/\n",
      ":            \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245519/\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "         \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1064315/\n",
      "1947        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1192684/\n",
      "27      \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1245611/\n",
      "        \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1058362/\n",
      "           \n",
      "\t--> Failed to scrape article due to: 403 Client Error: Forbidden for url: https://www.dawnnews.tv/news/1197185/\n",
      "\t--> Successfully scraped 15 articles from 'world'.\n"
     ]
    }
   ],
   "source": [
    "express_df = scraper.get_express_articles()\n",
    "dunya_df = scraper.get_dunya_articles()\n",
    "geo_df = scraper.get_geo_articles()\n",
    "jang_df = scraper.get_jang_articles()\n",
    "dawn_df = scraper.get_dawn_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 5 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat([express_df, dunya_df, geo_df, jang_df, dawn_df], ignore_index=True)\n",
    "combined_df.to_csv('combined_news_articles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
