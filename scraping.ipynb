{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "    def get_express_articles(self, max_pages=14):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "                        express_df[\"news_channel\"].append(\"Express News\")  # Optional\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_dunya_articles(self, max_pages=14):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.dunyanews.tv'\n",
    "        categories = ['Entertainment', 'Pakistan', 'World', 'Sports', 'Business']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/index.php/ur/{category}?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                news_boxes = soup.find_all(\"div\", class_=\"cNewsBox\")\n",
    "                print(f\"\\t--> Found {len(news_boxes)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for news in news_boxes:\n",
    "                    try:\n",
    "                        title_tag = news.find(\"h3\")\n",
    "                        if title_tag:\n",
    "                            link_tag = title_tag.find(\"a\")\n",
    "                            if link_tag:\n",
    "                                title = link_tag.get_text(strip=True)\n",
    "                                link = base_url + link_tag['href']\n",
    "                            else:\n",
    "                                print(\"\\t--> Skipping article due to missing link.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title tag.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        content = \"\"\n",
    "                        content_div = content_soup.find(\"div\", class_=\"main-news\") \n",
    "                        if content_div:\n",
    "                            content_paras = content_div.find_all(\"p\")\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                                for p in content_paras if p.get_text(strip=True)\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        dunya_df['id'].append(self.id)\n",
    "                        dunya_df['title'].append(title)\n",
    "                        dunya_df['link'].append(link)\n",
    "                        dunya_df['gold_label'].append(category)\n",
    "                        dunya_df['content'].append(content)\n",
    "                        dunya_df[\"news_channel\"].append(\"Dunya News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(dunya_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_geo_articles(self, max_pages=14):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv/category'\n",
    "        categories = ['business']\n",
    "        # categories = ['business', 'entertainment', 'sports', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/page/{page}/\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 403:\n",
    "                    print(\"Request was blocked by the server.\")\n",
    "                    break\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find_all(\"div\", class_=\"m_pic\")\n",
    "\n",
    "                print(f\"\\t--> Found {len(articles)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        title_tag = article.find(\"a\", class_=\"open-section\")\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get(\"title\", \"\").strip()\n",
    "                            link = title_tag[\"href\"]\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        \n",
    "                        content_div = content_soup.find(\"div\", class_=\"content-area\")\n",
    "                        content = \"\"\n",
    "                        if content_div:\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True)\n",
    "                                for p in content_div.find_all(\"p\")\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        geo_df[\"id\"].append(self.id)\n",
    "                        geo_df[\"title\"].append(title)\n",
    "                        geo_df[\"link\"].append(link)\n",
    "                        geo_df[\"gold_label\"].append(category.capitalize())\n",
    "                        geo_df[\"content\"].append(content)\n",
    "                        geo_df[\"news_channel\"].append(\"Geo News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_jang_articles(self):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "\n",
    "        base_url = 'https://jang.com.pk/category/latest-news'\n",
    "        #categories = ['entertainment', 'sports', 'world', 'health-science']\n",
    "        categories = ['health-science']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('ul',class_='scrollPaginationNew__').find_all(\"li\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    if article.get(\"class\") == [\"ad_latest_stories\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_pic = article.find(\"div\", class_=\"main-pic\")\n",
    "                    if main_pic:\n",
    "                        link_tag = main_pic.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.get(\"title\", \"\").strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        main_heading = article.find(\"div\", class_=\"main-heading\")\n",
    "                        if main_heading:\n",
    "                            link_tag = main_heading.find(\"a\", href=True)\n",
    "                            if link_tag:\n",
    "                                link = link_tag[\"href\"]\n",
    "                                title_tag = link_tag.find(\"h2\")\n",
    "                                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"detail_view_content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    jang_df[\"id\"].append(self.id)\n",
    "                    jang_df[\"title\"].append(title)\n",
    "                    jang_df[\"link\"].append(link)\n",
    "                    jang_df[\"gold_label\"].append(category.capitalize())\n",
    "                    jang_df[\"content\"].append(content)\n",
    "                    jang_df[\"news_channel\"].append(\"Jang\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(jang_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_dawn_articles(self):\n",
    "        dawn_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.dawnnews.tv/'\n",
    "        categories = ['business','sport', 'tech', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('div',class_='flex flex-row w-auto').find_all(\"article\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_div = article.find(\"h2\", class_=\"story__title\")\n",
    "                    if main_div:\n",
    "                        link_tag = main_div.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.text.strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"story__content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    dawn_df[\"id\"].append(self.id)\n",
    "                    dawn_df[\"title\"].append(title)\n",
    "                    dawn_df[\"link\"].append(link)\n",
    "                    dawn_df[\"gold_label\"].append(category.capitalize())\n",
    "                    dawn_df[\"content\"].append(content)\n",
    "                    dawn_df[\"news_channel\"].append(\"Dawn News\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(dawn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 18 articles on page 1 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 18 articles on page 2 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 18 articles on page 3 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 18 articles on page 4 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 18 articles on page 5 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 18 articles on page 6 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 18 articles on page 7 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 7 of 'business'.\n",
      "Scraping page 8 of category 'business'...\n",
      "\t--> Found 18 articles on page 8 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 8 of 'business'.\n",
      "Scraping page 9 of category 'business'...\n",
      "\t--> Found 18 articles on page 9 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 9 of 'business'.\n",
      "Scraping page 10 of category 'business'...\n",
      "\t--> Found 18 articles on page 10 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 8 articles from page 10 of 'business'.\n",
      "Scraping page 11 of category 'business'...\n",
      "\t--> Found 19 articles on page 11 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 11 of 'business'.\n",
      "Scraping page 12 of category 'business'...\n",
      "\t--> Found 19 articles on page 12 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 12 of 'business'.\n",
      "Scraping page 13 of category 'business'...\n",
      "\t--> Found 19 articles on page 13 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 13 of 'business'.\n",
      "Scraping page 14 of category 'business'...\n",
      "\t--> Found 19 articles on page 14 of 'business'.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Skipping article due to missing title or link.\n",
      "\t--> Successfully scraped 9 articles from page 14 of 'business'.\n",
      "\n",
      "Scraping category 'health-science'...\n",
      "\t--> Found 102 articles in 'health-science'.\n",
      "فضائی آلودگی سے پریشان ہیں تو زیتون کے پتوں کی بھاپ لیجیے\n",
      "پی ایم ڈی سی کی میڈیکل اور ڈینٹل کالجز میں اینٹی ہراسمنٹ کمیٹیاں بنانے کی ہدایت\n",
      "ناریل پانی کس موسم میں اور کس وقت زیادہ مؤثر؟\n",
      "کوہاٹ: دو بچوں میں پولیو وائرس کی تصدیق\n",
      "دن میں ساڑھے 10 گھنٹوں سے زیادہ بیٹھنا دل کیلئے نقصاندہ اور خطرناک ہوتا ہے، ماہرین\n",
      "نشتر اسپتال ملتان میں ایڈز منتقلی کی تحقیقات مکمل، رپورٹ سیکریٹری ہیلتھ کو پیش\n",
      "اینٹی مائیکروبیل ریزسٹنس سے نمٹنے کیلئے نیشنل ایکشن پلان ٹو تیار\n",
      "ملتان: نشتر اسپتال میں ڈائیلسز کے نئے مریضوں کے علاج پر پابندی\n",
      "پُرکشش دکھائی دینا ہے تو دن کا آغاز صحت بخش ناشتے سے کیجیے\n",
      "کیا کھانے کی خواہش ذہنی صحت پر اثرانداز ہوتی ہے؟\n",
      "ملتان: نشتر اسپتال میں ڈائیلسز کے رجسٹرڈ مریضوں کا ایک سال سے ایڈز ٹیسٹ نہ ہونے کا انکشاف\n",
      "ملتان: نشتر اسپتال میں 25 مریضوں میں ایڈز وائرس منتقل ہونے کی تصدیق\n",
      "کافی کے صحت پر حیرت انگیز فائدے، پھر بھی احتیاط ضروری\n",
      "پنجاب میں اسموگ کے باعث 19 لاکھ سے زائد افراد اسپتال پہنچ گئے\n",
      "تھوڑا آرام اپنے پیروں اور ایڑیوں کو بھی دیں\n",
      "ڈیمینشیا کی علامات برسوں پہلے ظاہر ہونا شروع ہوجاتی ہیں، تحقیق\n",
      "بلوچستان کے ضلع جعفر آباد سے پولیو کیس رپورٹ، تعداد 49 ہوگی\n",
      "سورج کی روشنی، دودھ اور مچھلی کے استعمال سے وٹامن ڈی کی کمی پوری ہوسکتی ہے، ماہرین\n",
      "بلوچستان میں پولیو کا ایک اور کیس رپورٹ\n",
      "سینیٹر فوزیہ ارشد کی ڈریپ ایکٹ میں ترمیم کی حمایت\n",
      "ملتان: نشتر اسپتال میں مریضوں میں ایڈز پھیلنے کی وجہ سامنے آ گئی\n",
      "پاکستان سمیت آج دنیا بھر میں ذیابیطس کا عالمی دن منایا جارہا ہے\n",
      "148 دن میں ساڑھے 53 لاکھ مریضوں کا انکی دہلیز پر علاج کیا گیا: مریم نواز\n",
      "ملتان: نشتر اسپتال میں 30 مریض ایڈز میں مبتلا\n",
      "گلے میں خراش یا سوزش ہے تو چند آزمودہ نسخے آپ کیلئے\n",
      "کراچی: مبینہ غلط انجکشن سے 3 سالہ بچی کا انتقال\n",
      "اسموگ سے بچوں کے پھپھڑے اور دماغی نشوونما متاثر ہو رہے ہیں: عالمی ادارۂ اطفال کا انتباہ\n",
      "بال خوبصورت، لمبے اور گھنے بنائیں گھر پر تیار شیمپو سے\n",
      "روزانہ کی خوراک سے صرف ایک گرام نمک کم کرنے کے فوائد\n",
      "رواں سال کراچی میں ڈینگی کے 1 ہزار 843 کیسز\n",
      "سندھ: رواں سال ڈینگی کے 2 ہزار 192 کیسز رپورٹ\n",
      "موسمبی سے صحت بھی، خوبصورتی بھی\n",
      "پنجاب میں ڈینگی کے 79 کیسز رپورٹ\n",
      "خوشی حاصل کرنے کی خواہش کا ڈوپامین سے کیا تعلق؟ اسے کنٹرول کیسے کریں؟\n",
      "لاہور میں اسموگ کی شدت برقرار، بیماریاں پھیلنے لگیں\n",
      "پاکستان میں 48ویں پولیو وائرس کیس کی تصدیق\n",
      "انسانوں کی عمر کتنی طویل ہو سکتی ہے؟ سائنسدانوں نے پتہ لگا لیا\n",
      "ایم ڈی کیٹ کے ہی مخالف ہوں: ڈاکٹر عذرا فضل پیچوہو\n",
      "قوتِ برداشت میں کمی محسوس ہو تو کیا کریں؟\n",
      "کونسی غذائیں ڈپریشن کم کرتی ہیں؟\n",
      "کوئٹہ: ادویات چھپانے پر 3 فارماسسٹس گرفتار، 2 کی تلاش جاری\n",
      "پنجاب: اسموگ سے آنکھ، ناک، کان، گلے کی بیماریاں پھیلنے لگیں\n",
      "ملک میں ایک اور پولیو کیس کی تصدیق، تعداد 46 ہو گئی\n",
      "بلوچستان: وزیرِ اعلیٰ کا انسدادِ پولیو کیلئے 10 روز میں جامع روڈ میپ مرتب کرنے کا حکم\n",
      "بلوچستان میں انسدادِ پولیو مہم اختتام پزیر، 99 فیصد ہدف حاصل\n",
      "پنجاب: ڈینگی کے مزید 118 کیسز رپورٹ\n",
      "خناق کیا ہے، بچاؤ کیلئے ویکسینیشن کیوں ضروری؟\n",
      "دادو: 7 سالہ بچی میں پولیو وائرس کا شبہ، کراچی کے اسپتال منتقل\n",
      "پنجاب بھر میں ڈینگی کیسز کے اعداد و شمار جاری\n",
      "51 فیصد افراد کے اینٹی بائیوٹک ادویات از خود استعمال کرنے کا انکشاف\n",
      "پنجاب میں ڈینگی وائرس کے مزید 141 کیسز رپورٹ\n",
      "کراچی: اکتوبر میں ڈینگی کے 363 کیسز رپورٹ\n",
      "ورزش سے قبل درست غذاؤں کا انتخاب کیوں ضروری؟\n",
      "ملک میں پولیو وائرس کے مزید دو کیسز سامنے آگئے\n",
      "بینائی کی حفاظت آپ کے ہاتھوں میں، اس کا خیال رکھیں\n",
      "گزشتہ برس دنیا بھر میں تپ دق کے 82 لاکھ نئے کیسز کی تشخیص ہوئی، عالمی ادارہ صحت\n",
      "اسلام آباد میں چکن گونیا کا پہلا تصدیق شدہ کیس رپورٹ\n",
      "پاکستان میں تپِ دق کے کیسز میں نمایاں اضافہ\n",
      "وزن کم کرنے کیلئے ورزش سے قبل کیا کھائیں اور کیا نہیں؟\n",
      "اورکزئی: حملے کے بعد رکی انسدادِ پولیو مہم کا آج سے دوبارہ آغاز\n",
      "بلوچستان: رواں سال پولیو کے 21 کیسز رپورٹ ہوئے\n",
      "ڈینگی پھیلانے والا مچھر ہی چکن گونیا کا باعث بنتا ہے، ماہرین\n",
      "گورنر سندھ کی ملیریا کے کیسز میں اضافے پر ایکشن کی ہدایت\n",
      "بلوچستان میں انسدادِ پولیو مہم جاری\n",
      "سندھ بھر میں ملیریا کے کیسز میں مسلسل اضافہ\n",
      "موبائل کا غلط انداز میں استعمال ذہنی مسائل کا سبب بن سکتا ہے، تحقیق\n",
      "پنجاب حکومت کا 150 بنیادی مراکز صحت آؤٹ سورس کرنے کافیصلہ\n",
      "کے پی کے: ڈینگی کے مزید 56 کیسز رپورٹ\n",
      "پولیو قطرے پلانے سے انکار کرنیوالے 50 فیصد لوگوں کی نشاندہی کر لی: مراد علی شاہ\n",
      "کے پی کے: 33 اضلاع میں 5 روزہ انسدادِ پولیو مہم کا پہلا مرحلہ شروع\n",
      "ایک سیب روزانہ، واقعی رکھے ڈاکٹر سے دور؟\n",
      "سندھ: 7 روزہ انسدادِ پولیو مہم کا آغاز آج ہو گا\n",
      "سندھ بھر میں سات روزہ انسداد پولیو مہم کا آغاز کل سے ہوگا، ایمرجنسی آپریشن سینٹر\n",
      "لاہور میں آلودگی سے بیماریاں پھیلنے لگیں\n",
      "پولیو سے نمٹنے کیلئے پاکستان و دیگر ممالک میں سعودی منصوبوں پر عملدرآمد\n",
      "’سُپر فوڈ‘ پالک, کئی موذی امراض کے خلاف مؤثر\n",
      "کراچی: شارجہ سے آنے والی پرواز کے مسافر میں منکی پاکس کی علامات\n",
      "پاکستان میں ایک اور بچہ پولیو سے معذور ہوگیا\n",
      "کراچی میں ڈینگی کے کیسز میں اضافہ، آج شہر میں ڈینگی کے 18 کیسز رپورٹ\n",
      "کراچی: رواں سال ملیریا کے 1 ہزار 935، چکن گونیا کے 172 کیسز رپورٹ\n",
      "کراچی: رواں سال ڈینگی کے 1647 کیسز سامنے آ گئے\n",
      "پنجاب: ڈینگی کے مزید 128 کیسز رپورٹ، 1 مریض کا انتقال\n",
      "چکن گونیا سے بچاؤ کیسے ممکن؟\n",
      "لاہور: فضائی آلودگی بڑھ گئی، بیماریاں پھیلنے کا خطرہ، شہری کیا احتیاطی تدابیر اختیار کریں؟\n",
      "علماء پولیو سے متعلق آگاہی پھیلانے میں کردار ادا کریں: علی امین گنڈاپور\n",
      "پاکستان میں پہلی پولیو ویکسین آصفہ بھٹو کو پلائی گئی: بلاول بھٹو\n",
      "ملک میں پولیو وائرس کا 40 واں کیس رپورٹ\n",
      "لاہور کی فضا میں آلودگی مزید بڑھ گئی\n",
      "خوبصورت اور صحت مند بال اب خواب نہیں\n",
      "مٹھی میں مختلف بیماریوں کے باعث 24 گھنٹوں میں 6 بچے جاں بحق\n",
      "کراچی: نومبر میں ڈینگی کیسز کم ہونے کا امکان\n",
      "چھاتی کے سرطان کے سالانہ 38000 کیسز رپورٹ ہوتے ہیں: ترجمان سندھ حکومت\n",
      "بلوچستان میں رواں سال سب سے زیادہ 20 پولیو کیسز رپورٹ\n",
      "بریسٹ کینسر کیوں ہوتا ہے؟ بچاؤ کیسے ممکن؟\n",
      "عالمی ادارہ صحت نے مصر کو ملیریا سے پاک قرار دے دیا\n",
      "کراچی: 19 اکتوبر کو ڈینگی کے 10 کیسز رپورٹ ہوئے\n",
      "ملک میں رواں سال پولیو سے 3 بچے جاں بحق\n",
      "سندھ میں پولیو وائرس کے مزید 2 کیسز کی تصدیق، ملک بھر میں تعداد 39 ہوگئی\n",
      "پنجاب کو مکمل پولیو فری بنانا ہمارا عزم ہے: وزیرِ اعلیٰ پنجاب\n",
      "کراچی: بچوں میں نمونیا کیسز میں اضافہ\n",
      "\t--> Successfully scraped 100 articles from 'health-science'.\n"
     ]
    }
   ],
   "source": [
    "# express_df = scraper.get_express_articles()\n",
    "# dunya_df = scraper.get_dunya_articles()\n",
    "geo_df = scraper.get_geo_articles()\n",
    "jang_df = scraper.get_jang_articles()\n",
    "# dawn_df = scraper.get_dawn_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 5 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [],
   "source": [
    "old_df = pd.read_csv('data/data5.csv')\n",
    "combined_df = pd.concat([old_df,jang_df,geo_df], ignore_index=True)\n",
    "combined_df.to_csv('data6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
