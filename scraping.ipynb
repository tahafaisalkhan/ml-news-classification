{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "    def get_express_articles(self, max_pages=14):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "                        express_df[\"news_channel\"].append(\"Express News\")  # Optional\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_dunya_articles(self, max_pages=14):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.dunyanews.tv'\n",
    "        categories = ['Entertainment', 'Pakistan', 'World', 'Sports', 'Business']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/index.php/ur/{category}?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                news_boxes = soup.find_all(\"div\", class_=\"cNewsBox\")\n",
    "                print(f\"\\t--> Found {len(news_boxes)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for news in news_boxes:\n",
    "                    try:\n",
    "                        title_tag = news.find(\"h3\")\n",
    "                        if title_tag:\n",
    "                            link_tag = title_tag.find(\"a\")\n",
    "                            if link_tag:\n",
    "                                title = link_tag.get_text(strip=True)\n",
    "                                link = base_url + link_tag['href']\n",
    "                            else:\n",
    "                                print(\"\\t--> Skipping article due to missing link.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title tag.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        content = \"\"\n",
    "                        content_div = content_soup.find(\"div\", class_=\"main-news\") \n",
    "                        if content_div:\n",
    "                            content_paras = content_div.find_all(\"p\")\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                                for p in content_paras if p.get_text(strip=True)\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        dunya_df['id'].append(self.id)\n",
    "                        dunya_df['title'].append(title)\n",
    "                        dunya_df['link'].append(link)\n",
    "                        dunya_df['gold_label'].append(category)\n",
    "                        dunya_df['content'].append(content)\n",
    "                        dunya_df[\"news_channel\"].append(\"Dunya News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(dunya_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_geo_articles(self, max_pages=14):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv/category'\n",
    "        categories = ['business', 'entertainment', 'sports', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/page/{page}/\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 403:\n",
    "                    print(\"Request was blocked by the server.\")\n",
    "                    break\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find_all(\"div\", class_=\"m_pic\")\n",
    "\n",
    "                print(f\"\\t--> Found {len(articles)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        title_tag = article.find(\"a\", class_=\"open-section\")\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get(\"title\", \"\").strip()\n",
    "                            link = title_tag[\"href\"]\n",
    "                        else:\n",
    "                            print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                            continue\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        \n",
    "                        content_div = content_soup.find(\"div\", class_=\"content-area\")\n",
    "                        content = \"\"\n",
    "                        if content_div:\n",
    "                            content = \" \".join(\n",
    "                                p.get_text(strip=True)\n",
    "                                for p in content_div.find_all(\"p\")\n",
    "                            )\n",
    "\n",
    "                        if not content:\n",
    "                            print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                            continue\n",
    "\n",
    "                        geo_df[\"id\"].append(self.id)\n",
    "                        geo_df[\"title\"].append(title)\n",
    "                        geo_df[\"link\"].append(link)\n",
    "                        geo_df[\"gold_label\"].append(category.capitalize())\n",
    "                        geo_df[\"content\"].append(content)\n",
    "                        geo_df[\"news_channel\"].append(\"Geo News\")  # Optional\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_jang_articles(self):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "\n",
    "        base_url = 'https://jang.com.pk/category/latest-news'\n",
    "        categories = ['entertainment', 'sports', 'world', 'health-science']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('ul',class_='scrollPaginationNew__').find_all(\"li\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    if article.get(\"class\") == [\"ad_latest_stories\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_pic = article.find(\"div\", class_=\"main-pic\")\n",
    "                    if main_pic:\n",
    "                        link_tag = main_pic.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.get(\"title\", \"\").strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        main_heading = article.find(\"div\", class_=\"main-heading\")\n",
    "                        if main_heading:\n",
    "                            link_tag = main_heading.find(\"a\", href=True)\n",
    "                            if link_tag:\n",
    "                                link = link_tag[\"href\"]\n",
    "                                title_tag = link_tag.find(\"h2\")\n",
    "                                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"detail_view_content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    jang_df[\"id\"].append(self.id)\n",
    "                    jang_df[\"title\"].append(title)\n",
    "                    jang_df[\"link\"].append(link)\n",
    "                    jang_df[\"gold_label\"].append(category.capitalize())\n",
    "                    jang_df[\"content\"].append(content)\n",
    "                    jang_df[\"news_channel\"].append(\"Jang\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(jang_df)\n",
    "\n",
    "\n",
    "\n",
    "    def get_dawn_articles(self):\n",
    "        dawn_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "            \"news_channel\": [], # optional\n",
    "        }\n",
    "        base_url = 'https://www.dawnnews.tv/'\n",
    "        categories = ['business','sport', 'tech', 'world']\n",
    "\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            articles = soup.find('div',class_='flex flex-row w-auto').find_all(\"article\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title, link = None, None\n",
    "\n",
    "                    main_div = article.find(\"h2\", class_=\"story__title\")\n",
    "                    if main_div:\n",
    "                        link_tag = main_div.find(\"a\", href=True)\n",
    "                        if link_tag:\n",
    "                            link = link_tag[\"href\"]\n",
    "                            title = link_tag.text.strip()\n",
    "                            print(title)\n",
    "\n",
    "                    if not title or not link:\n",
    "                        print(\"\\t--> Skipping article due to missing title or link.\")\n",
    "                        continue\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    content_div = content_soup.find(\"div\", class_=\"story__content\")\n",
    "                    content = \"\"\n",
    "                    if content_div:\n",
    "                        content = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in content_div.find_all(\"p\")\n",
    "                        )\n",
    "\n",
    "                    if not content:\n",
    "                        print(f\"\\t--> Skipping article '{title}' due to missing content.\")\n",
    "                        continue\n",
    "\n",
    "                    dawn_df[\"id\"].append(self.id)\n",
    "                    dawn_df[\"title\"].append(title)\n",
    "                    dawn_df[\"link\"].append(link)\n",
    "                    dawn_df[\"gold_label\"].append(category.capitalize())\n",
    "                    dawn_df[\"content\"].append(content)\n",
    "                    dawn_df[\"news_channel\"].append(\"Dawn News\")  # Optional\n",
    "\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape article due to: {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(dawn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'saqafat'.\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "Scraping page 8 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 8 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'saqafat'.\n",
      "Scraping page 9 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 9 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'saqafat'.\n",
      "Scraping page 10 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 10 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'saqafat'.\n",
      "Scraping page 11 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 11 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 11 of 'saqafat'.\n",
      "Scraping page 12 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 12 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 12 of 'saqafat'.\n",
      "Scraping page 13 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 13 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 13 of 'saqafat'.\n",
      "Scraping page 14 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 14 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 14 of 'saqafat'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "Scraping page 8 of category 'business'...\n",
      "\t--> Found 10 articles on page 8 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'business'.\n",
      "Scraping page 9 of category 'business'...\n",
      "\t--> Found 10 articles on page 9 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'business'.\n",
      "Scraping page 10 of category 'business'...\n",
      "\t--> Found 10 articles on page 10 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'business'.\n",
      "Scraping page 11 of category 'business'...\n",
      "\t--> Found 10 articles on page 11 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 11 of 'business'.\n",
      "Scraping page 12 of category 'business'...\n",
      "\t--> Found 10 articles on page 12 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 12 of 'business'.\n",
      "Scraping page 13 of category 'business'...\n",
      "\t--> Found 10 articles on page 13 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 13 of 'business'.\n",
      "Scraping page 14 of category 'business'...\n",
      "\t--> Found 10 articles on page 14 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 14 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "Scraping page 8 of category 'sports'...\n",
      "\t--> Found 10 articles on page 8 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'sports'.\n",
      "Scraping page 9 of category 'sports'...\n",
      "\t--> Found 10 articles on page 9 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'sports'.\n",
      "Scraping page 10 of category 'sports'...\n",
      "\t--> Found 10 articles on page 10 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'sports'.\n",
      "Scraping page 11 of category 'sports'...\n",
      "\t--> Found 10 articles on page 11 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 11 of 'sports'.\n",
      "Scraping page 12 of category 'sports'...\n",
      "\t--> Found 10 articles on page 12 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 12 of 'sports'.\n",
      "Scraping page 13 of category 'sports'...\n",
      "\t--> Found 10 articles on page 13 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 13 of 'sports'.\n",
      "Scraping page 14 of category 'sports'...\n",
      "\t--> Found 10 articles on page 14 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 14 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "Scraping page 8 of category 'science'...\n",
      "\t--> Found 10 articles on page 8 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'science'.\n",
      "Scraping page 9 of category 'science'...\n",
      "\t--> Found 10 articles on page 9 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'science'.\n",
      "Scraping page 10 of category 'science'...\n",
      "\t--> Found 10 articles on page 10 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'science'.\n",
      "Scraping page 11 of category 'science'...\n",
      "\t--> Found 10 articles on page 11 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 11 of 'science'.\n",
      "Scraping page 12 of category 'science'...\n",
      "\t--> Found 10 articles on page 12 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 12 of 'science'.\n",
      "Scraping page 13 of category 'science'...\n",
      "\t--> Found 10 articles on page 13 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 13 of 'science'.\n",
      "Scraping page 14 of category 'science'...\n",
      "\t--> Found 10 articles on page 14 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 14 of 'science'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "Scraping page 8 of category 'world'...\n",
      "\t--> Found 10 articles on page 8 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'world'.\n",
      "Scraping page 9 of category 'world'...\n",
      "\t--> Found 10 articles on page 9 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'world'.\n",
      "Scraping page 10 of category 'world'...\n",
      "\t--> Found 10 articles on page 10 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'world'.\n",
      "Scraping page 11 of category 'world'...\n",
      "\t--> Found 10 articles on page 11 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 11 of 'world'.\n",
      "Scraping page 12 of category 'world'...\n",
      "\t--> Found 10 articles on page 12 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 12 of 'world'.\n",
      "Scraping page 13 of category 'world'...\n",
      "\t--> Found 10 articles on page 13 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 13 of 'world'.\n",
      "Scraping page 14 of category 'world'...\n",
      "\t--> Found 10 articles on page 14 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 14 of 'world'.\n",
      "\n",
      "Scraping page 1 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 1 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Entertainment'.\n",
      "Scraping page 2 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 2 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Entertainment'.\n",
      "Scraping page 3 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 3 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Entertainment'.\n",
      "Scraping page 4 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 4 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Entertainment'.\n",
      "Scraping page 5 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 5 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Entertainment'.\n",
      "Scraping page 6 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 6 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Entertainment'.\n",
      "Scraping page 7 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 7 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Entertainment'.\n",
      "Scraping page 8 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 8 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 8 of 'Entertainment'.\n",
      "Scraping page 9 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 9 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 9 of 'Entertainment'.\n",
      "Scraping page 10 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 10 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 10 of 'Entertainment'.\n",
      "Scraping page 11 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 11 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 11 of 'Entertainment'.\n",
      "Scraping page 12 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 12 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 12 of 'Entertainment'.\n",
      "Scraping page 13 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 13 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 13 of 'Entertainment'.\n",
      "Scraping page 14 of category 'Entertainment'...\n",
      "\t--> Found 18 articles on page 14 of 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles from page 14 of 'Entertainment'.\n",
      "\n",
      "Scraping page 1 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 1 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Pakistan'.\n",
      "Scraping page 2 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 2 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Pakistan'.\n",
      "Scraping page 3 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 3 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Pakistan'.\n",
      "Scraping page 4 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 4 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Pakistan'.\n",
      "Scraping page 5 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 5 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Pakistan'.\n",
      "Scraping page 6 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 6 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Pakistan'.\n",
      "Scraping page 7 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 7 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Pakistan'.\n",
      "Scraping page 8 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 8 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 8 of 'Pakistan'.\n",
      "Scraping page 9 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 9 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 9 of 'Pakistan'.\n",
      "Scraping page 10 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 10 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 10 of 'Pakistan'.\n",
      "Scraping page 11 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 11 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 11 of 'Pakistan'.\n",
      "Scraping page 12 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 12 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 12 of 'Pakistan'.\n",
      "Scraping page 13 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 13 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 13 of 'Pakistan'.\n",
      "Scraping page 14 of category 'Pakistan'...\n",
      "\t--> Found 18 articles on page 14 of 'Pakistan'.\n",
      "\t--> Successfully scraped 18 articles from page 14 of 'Pakistan'.\n",
      "\n",
      "Scraping page 1 of category 'World'...\n",
      "\t--> Found 18 articles on page 1 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'World'.\n",
      "Scraping page 2 of category 'World'...\n",
      "\t--> Found 18 articles on page 2 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'World'.\n",
      "Scraping page 3 of category 'World'...\n",
      "\t--> Found 18 articles on page 3 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'World'.\n",
      "Scraping page 4 of category 'World'...\n",
      "\t--> Found 18 articles on page 4 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'World'.\n",
      "Scraping page 5 of category 'World'...\n",
      "\t--> Found 18 articles on page 5 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'World'.\n",
      "Scraping page 6 of category 'World'...\n",
      "\t--> Found 18 articles on page 6 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'World'.\n",
      "Scraping page 7 of category 'World'...\n",
      "\t--> Found 18 articles on page 7 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'World'.\n",
      "Scraping page 8 of category 'World'...\n",
      "\t--> Found 18 articles on page 8 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 8 of 'World'.\n",
      "Scraping page 9 of category 'World'...\n",
      "\t--> Found 18 articles on page 9 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 9 of 'World'.\n",
      "Scraping page 10 of category 'World'...\n",
      "\t--> Found 18 articles on page 10 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 10 of 'World'.\n",
      "Scraping page 11 of category 'World'...\n",
      "\t--> Found 18 articles on page 11 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 11 of 'World'.\n",
      "Scraping page 12 of category 'World'...\n",
      "\t--> Found 18 articles on page 12 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 12 of 'World'.\n",
      "Scraping page 13 of category 'World'...\n",
      "\t--> Found 18 articles on page 13 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 13 of 'World'.\n",
      "Scraping page 14 of category 'World'...\n",
      "\t--> Found 18 articles on page 14 of 'World'.\n",
      "\t--> Successfully scraped 18 articles from page 14 of 'World'.\n",
      "\n",
      "Scraping page 1 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 1 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Sports'.\n",
      "Scraping page 2 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 2 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Sports'.\n",
      "Scraping page 3 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 3 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Sports'.\n",
      "Scraping page 4 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 4 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Sports'.\n",
      "Scraping page 5 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 5 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Sports'.\n",
      "Scraping page 6 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 6 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Sports'.\n",
      "Scraping page 7 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 7 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Sports'.\n",
      "Scraping page 8 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 8 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 8 of 'Sports'.\n",
      "Scraping page 9 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 9 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 9 of 'Sports'.\n",
      "Scraping page 10 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 10 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 10 of 'Sports'.\n",
      "Scraping page 11 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 11 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 11 of 'Sports'.\n",
      "Scraping page 12 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 12 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 12 of 'Sports'.\n",
      "Scraping page 13 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 13 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 13 of 'Sports'.\n",
      "Scraping page 14 of category 'Sports'...\n",
      "\t--> Found 18 articles on page 14 of 'Sports'.\n",
      "\t--> Successfully scraped 18 articles from page 14 of 'Sports'.\n",
      "\n",
      "Scraping page 1 of category 'Business'...\n",
      "\t--> Found 18 articles on page 1 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 1 of 'Business'.\n",
      "Scraping page 2 of category 'Business'...\n",
      "\t--> Found 18 articles on page 2 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 2 of 'Business'.\n",
      "Scraping page 3 of category 'Business'...\n",
      "\t--> Found 18 articles on page 3 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 3 of 'Business'.\n",
      "Scraping page 4 of category 'Business'...\n",
      "\t--> Found 18 articles on page 4 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 4 of 'Business'.\n",
      "Scraping page 5 of category 'Business'...\n",
      "\t--> Found 18 articles on page 5 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 5 of 'Business'.\n",
      "Scraping page 6 of category 'Business'...\n",
      "\t--> Found 18 articles on page 6 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 6 of 'Business'.\n",
      "Scraping page 7 of category 'Business'...\n",
      "\t--> Found 18 articles on page 7 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 7 of 'Business'.\n",
      "Scraping page 8 of category 'Business'...\n",
      "\t--> Found 18 articles on page 8 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 8 of 'Business'.\n",
      "Scraping page 9 of category 'Business'...\n",
      "\t--> Found 18 articles on page 9 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 9 of 'Business'.\n",
      "Scraping page 10 of category 'Business'...\n",
      "\t--> Found 18 articles on page 10 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 10 of 'Business'.\n",
      "Scraping page 11 of category 'Business'...\n",
      "\t--> Found 18 articles on page 11 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 11 of 'Business'.\n",
      "Scraping page 12 of category 'Business'...\n",
      "\t--> Found 18 articles on page 12 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 12 of 'Business'.\n",
      "Scraping page 13 of category 'Business'...\n",
      "\t--> Found 18 articles on page 13 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 13 of 'Business'.\n",
      "Scraping page 14 of category 'Business'...\n",
      "\t--> Found 18 articles on page 14 of 'Business'.\n",
      "\t--> Successfully scraped 18 articles from page 14 of 'Business'.\n",
      "\n",
      "Scraping category 'entertainment'...\n",
      "\t--> Found 101 articles in 'entertainment'.\n",
      "نوجوت سدھو نے کپل شرما شو سے الگ ہونے کی وجہ بتادی\n",
      "ان کو یہاں بلاکر غلطی کر دی، امیتابھ کا ابھیشیک کو شو میں بلانے پر افسوس\n",
      "زندگی پر بھروسہ نہیں کر سکتے، ممکن ہے کل مر جائیں، عامر خان\n",
      "ایشوریہ سے متعلق سلمان خان کی پرانی ویڈیو وائرل ہوگئی\n",
      "دلجیت دوسانجھ کو حیدرآباد میں کنسرٹ سے قبل پابندی کا سامنا\n",
      "دولہا آن لائن پارٹ 2: سید فرقان حیدر کے آخری شاہکار ڈرامے نے ڈیلس کا دل جیت لیا\n",
      "عدنان صدیقی کی شاہ چارلس سے ناقابلِ فراموش ملاقات\n",
      "انٹرنیٹ صارفین کی جانب سے کیارا ایڈوانی پر دیپیکا کی نقل کا الزام\n",
      "ڈیوڈ دھون نے وزن میں کمی کیلئے سرجری سے منع کیا تھا، ہما قریشی\n",
      "امیشا پٹیل کو 19 سال چھوٹا بوائے فرینڈ مل گیا، پوسٹ پر مداحوں کے دلچسپ تبصرے\n",
      "میکا سنگھ کو پاکستانی مداح کے کروڑوں کے تحائف\n",
      "فلم انڈسٹری سے ریٹائرمنٹ کا فیصلہ واپس کیوں لیا؟ عامر خان نے بتا دیا\n",
      "آج کل کی اداکاراؤں میں کام کا جنون نہیں، یہ نسل شارٹ کٹ پر یقین رکھتی ہے: لیلیٰ زبیری\n",
      "وینا ملک رشتۂ ازدواج میں منسلک ہو گئیں؟\n",
      "بالی ووڈ کا سب سے امیر خاندان جو کبھی فروٹ بیچا کرتا تھا\n",
      "16 برس کی عمر میں بےہوش کرکے غیراخلاقی حرکت کی کوشش کی گئی، رشامی ڈیسائی\n",
      "ہالی ووڈ کی سب سے بڑی فلاپ فلم، 265 ملین ڈالرز کا نقصان\n",
      "ٹرمپ کے مشورے پر بھتہ خور نے محبوبہ جیکولین فرنینڈس کیلئے لاس اینجلس میں فلم اسٹوڈیو خرید لیا\n",
      "شوہر دوسری شادی کرنا چاہتے ہیں تو مجھے اعتراض نہیں: حرا سومرو\n",
      "امیتابھ بچن کا نمرت کور کو خط توجہ کا مرکز بن گیا\n",
      "بھارتی اداکارہ نے سوتیلی بیٹی کیخلاف ہتک عزت کا دعویٰ دائر کردیا\n",
      "سلمان خان کو دھمکی دینے کے کیس میں نوجوان گیت نگار گرفتار\n",
      "عاطف اسلم کا سعودی عرب میں کنسرٹ سے متعلق بیان سے اظہار لاتعلقی\n",
      "شاہد کپور نے ممبئی میں اپنا پرتعیش اپارٹمنٹ 20 لاکھ روپے کرایہ پر دیدیا\n",
      "جنوبی کوریا کے 39 سالہ اداکار اپنے اپارٹمنٹ میں مردہ پائے گئے\n",
      "لاہور: میشا شفیع کے خلاف 100کروڑ کے ہتک عزت کے دعوے پر سماعت 26 نومبر تک ملتوی\n",
      "یشما گِل کو’ دل کا رشتہ ایپ‘ پر رشتے  کیلئے 10 ہزار درخواستیں آ گئیں\n",
      "شاہ رخ خان کو قتل کی دھمکی، وکیل گرفتار\n",
      "خالہ روحی بانو کے انتقال کا سن کر سکون ملا کہ وہ تکلیف بھری زندگی سے دور ہو گئیں: فریال محمود\n",
      "مشی خان لائیو شو کے دوران سیلفی لیتے ہوئے صوفے سے گر گئیں\n",
      "شردھا کپور نے نوجوان کے ویڈیو تبصرے پر قہقہے لگادیے\n",
      "چندرا بابو سے متعلق ریمارکس، رام گوپال ورما پر مقدمہ درج\n",
      "نوجوت سدھو کی 5 برس بعد کپل شرما شو میں واپسی؟ ارچنا پریشان\n",
      "اسٹیج ڈرامہ پروڈیوسر فرقان حیدر رضوی کا انتقال سر پر چوٹ لگنے سے ہوا، ڈیلس میں سپردِ خاک\n",
      "تمام مذاہب کا مطالعہ کرنے کے بعد اسلام کا انتخاب کیا: حمزہ علی عباسی\n",
      "شادی سے متعلق سوال پر ہانیہ عامر نے کیا کہا؟\n",
      "عامر اور سلمان کی انداز اپنا اپنا کو 30 برس مکمل، اداکاروں نے فلم کے بارے میں کیا کہا تھا؟\n",
      "بھارتی اداکار نے بگ باس کو بہت زیادہ تذلیل کرنے والا شو کہہ دیا\n",
      "بھول بھلیاں کی کامیابی سے فلمسازوں کا میرے بارے میں تصور تبدیل ہوا، ودیا بالن\n",
      "پاکستان میں ایوارڈ سفارش یا تعلقات پر ملتے ہیں، فرقان حیدر\n",
      "نعمان اعجاز اور صبا قمر بولڈ سین فلمانے پر شدید تنقید کی زد میں\n",
      "دلجیت سنگھ دوسانجھ ابوظبی میں شیخ زید مسجد پہنچ گئے\n",
      "معروف اسٹیج ڈرامہ پروڈیوسر سید فرقان حیدر رضوی انتقال کرگئے\n",
      "یشمہ گل رشتے والی آنٹیوں کے بے جا مطالبات سے تنگ آگئیں\n",
      "شادی کے دوران جان سے مارنے کی دھمکیاں دی گئی تھیں، سیف علی خان کا انکشاف\n",
      "’وہ پہلے دودھ والا تھا‘، اجے دیوگن کا اکشے کی جلدی اٹھنے کی عادت پر دلچسپ تبصرہ\n",
      "کنگنا رناوت کی نانی چل بسیں، اداکارہ نے پرانی یادیں تازہ کردیں\n",
      "بھارت میں جسے ناچ گانا نہیں آتا اس کا رشتہ نہیں ہوتا، یہاں جسے آتا ہے اس کی شادی نہیں ہوتی: اداکارہ دیدار\n",
      "صدارتی ایوارڈ یافتہ 87 سالہ سرائیکی شاعر اقبال سوکڑی انتقال کر گئے\n",
      "ہنی سنگھ کے گانے ’جٹ مہکما‘ میں مہوش حیات لیڈی ڈان بن گئیں، ویڈیو ریلیز\n",
      "افتخار ٹھاکر کا عیسائی نوجوان کو اسلام قبول کروانے کا دلچسپ واقعہ\n",
      "ٹرمپ کی کامیابی کے بعد کئی ہالی ووڈ اداکاراؤں کا امریکا چھوڑنے کا ارادہ\n",
      "ژالے سرحدی نے ’گرو جی‘ کی آڈیو پر اداکاری کر کے محفل لوٹ لی\n",
      "سلمان خان تم گیت نگار کو بچاسکتے ہو تو بچالو، بشنوئی گینگ کی نئی دھمکی\n",
      "مشہور بھارتی اداکارہ نے ایکٹنگ چھوڑکر 1300 کروڑ کی کمپنی بنالی\n",
      "کاروباری مشوروں کے ساتھ والد مارکیٹنگ کے معاملے میں ذہن ترین ہیں، آریان خان\n",
      "معروف فیشن ڈیزائنر محمود بھٹی میٹ دی پریس میں آبدیدہ ہوگئے\n",
      "سوشل میڈیا انفلوئنسرز نوجوان لڑکیوں کے دماغ خراب کر رہے ہیں: زویا ناصر\n",
      "سلمان خان کو جان سے مارنے کی دھمکی دینے والا شخص گرفتار\n",
      "والدین نے سدھو موسےوالا کے چھوٹے بھائی کی تصویر جاری کردی\n",
      "منیشا کوئرالہ نے مادھوری ڈکشٹ کی ہم شکل کہلانے پر اپنا ردِ عمل دے دیا\n",
      "شاہ رخ کو دھمکی آمیز کال سے کوئی تعلق نہیں، ایڈووکیٹ فیضان\n",
      "ہنی سنگھ نے سرائیکی گانا پیش کر کے دل جیت لیے\n",
      "ریچا چڈھا اور علی فضل نے 4 ماہ بعد بیٹی کا نام رکھ دیا\n",
      "شاہ رخ خان کو دھمکی کی کال پولیس کو موصول\n",
      "طوبیٰ انور کو کیسے جیون ساتھی کی تلاش؟\n",
      "برطانوی پارلیمنٹ نے ماہرہ خان کو اچیومنٹ ایوارڈ سے نواز دیا\n",
      "کم عمر میں شادی کیوں کی؟ نادیہ خان نے وجہ بتا دی\n",
      "ابھیشیک اور ایشوریا فلم کیلئے اکٹھے، طلاق کی افواہوں کے درمیان نیا پروجیکٹ\n",
      "ایشوریا سے طلاق کی افواہیں، سیمی گریوال کو ابھیشیک کا دفاع کرنا مہنگا پڑ گیا\n",
      "کینسر کی تشخیص کے بعد لگا مرنے والی ہوں، منیشا کوئرالہ\n",
      "امریکی شہری ہوتیں تو کسے ووٹ دیتیں؟ کنگنا رناوت نے بتا دیا\n",
      "ماہرہ خان کی ساڑھی میں نئی تصاویر وائرل\n",
      "امریکی صدارتی انتخابات: ریما نے بھی ووٹ کاسٹ کیا\n",
      "چاہت فتح کے ’بدوبدی‘ پر ماضی کی اداکارہ ممتاز ناراض، بیٹے کا مختلف مؤقف\n",
      "احسن خان کا کرس گیل کیساتھ ڈانس، ویڈیو وائرل\n",
      "انوشکا شرما نے بیٹے کی پہلی تصویر شیئر کر دی\n",
      "کرینہ کا مداح کو سیلفی لینے سے انکار، سوشل میڈیا صارفین کا ملاجلا ردعمل\n",
      "آرٹس کونسل کراچی نے عوامی تھیٹر فیسٹیول کی روایت کو برقرار رکھا ہوا ہے، محمد احمد شاہ\n",
      "سلمان مندر میں آکر معافی مانگیں یا 5 کروڑ ادا کریں، بشنوئی گینگ کی دھمکی\n",
      "62 بہاریں دیکھنے والی زیبا بختیار نے خوبصورتی کا راز بتا دیا\n",
      "مسرت مصباح کی طلاق پر انکے والدین کا کیا ردِعمل تھا؟\n",
      "سکندر صنم کو بچھڑے 12 برس ہو گئے\n",
      "میرے شوہر نے مجھے ایک ملکہ کی طرح رکھا: عینی زیدی\n",
      "کب شادی کریں گی؟ مہوش حیات  نے بتا دیا\n",
      "ماہرہ خان کو شاہ رخ خان کی کونسی عادت بے حد پسند؟\n",
      "بھارتی اداکارہ کا مدینہ منورہ میں نکاح\n",
      "متھن چکرورتی کی پہلی بیوی ہلینا لیوک امریکا میں انتقال کرگئیں\n",
      "ذیشان صدیقی نے سلمان خان، شاہ رخ خان سے تعلق کی نوعیت بتادی\n",
      "شاہ رخ خان کی تین ماہ سے منت کے باہر بیٹھے فین سے ملاقات\n",
      "امریکی میوزک پروڈیوسر و موسیقار کوئنسی جونز انتقال کرگئے\n",
      "عمیر جیسوال نے دوسری اہلیہ کو خفیہ رکھنے کی وجہ بتا دی\n",
      "میں نے سگریٹ چھوڑ دی: شاہ رخ خان\n",
      "راجپال یادیو صحافی کے سوال پر غصہ، موبائل فون چھین لیا\n",
      "تاپسی پنوں کا فلم ’ڈنکی‘ میں کام سے متعلق بڑا انکشاف\n",
      "عامر خان کی بری عادت فیصلے کرنے میں بہت زیادہ وقت لگانا ہے، کرن رائو\n",
      "اگر بچوں کی لڑائی ہوئی تو شاہ رخ خان کس کا ساتھ دیں گے؟\n",
      "شاہ رخ خان نے ممبئی پولیس کو کھانا بھجوا دیا\n",
      "38 روزہ ”ورلڈ کلچر فیسٹیول کراچی“ ثقافتی رعنائیاں بکھیرتے ہوئے اختتام پذیر\n",
      "\t--> Successfully scraped 99 articles from 'entertainment'.\n",
      "Scraping category 'sports'...\n",
      "\t--> Found 102 articles in 'sports'.\n",
      "چیمپنز ٹرافی شیڈول میں تاخیر، آئی سی سی کمرشل پارٹنرز میں بے چینی بڑھ گئی\n",
      "سوال گندم، جواب چنا، پی ایس بی نے پی ایف ایف کا جواب مسترد کردیا\n",
      "جونیئر ہاکی ایشیا کپ کیلئے پاکستانی ٹیم کا اعلان\n",
      "چیمپئنز ٹرافی کے پاکستان ٹور پر بھارتی اعتراض ناجائز ہے، ذرائع\n",
      "آئی سی سی نے بھارتی اعتراض پر پاکستان کو چیمپئنز ٹرافی کشمیر لیجانے سے روک دیا، بھارتی میڈیا\n",
      "اشک آباد اوپن ٹینس: محمد حسن عثمانی سیمی فائنل میں پہنچ گئے\n",
      "بھارتی کرکٹ ٹیم کے پاکستان نہ آنے سے 9 ویں ٹیم کی شمولیت کا امکان\n",
      "چیمپئنز ٹرافی پر پی سی بی کا مؤقف قابلِ تعریف ہے: محمد عامر\n",
      "انگلینڈ کے خلاف تین ٹیسٹ میچز کی سیریز کیلئے نیوزی لینڈ کرکٹ ٹیم کا اعلان\n",
      "نیوزی لینڈ کے فاسٹ بولر ٹم ساؤتھی نے ٹیسٹ کرکٹ سے ریٹائرمنٹ کا اعلان کردیا\n",
      "افغانستان ویمن کرکٹ ٹیم کے نمائشی میچ کا اعلان\n",
      "بہترین پلیئنگ الیون کیلئے اچھےکھلاڑی تلاش کر رہے ہیں، محمد رضوان\n",
      "آئی سی سی نے چیمپئنز ٹرافی دبئی سے اسلام آباد پہنچادی\n",
      "انٹرنیشنل کرکٹ کونسل نے تاحال پی سی بی کے خط کا جواب نہیں دیا\n",
      "قائدِاعظم ٹرافی کے میچ کا نتیجہ ڈیڑھ دن میں آ گیا\n",
      "پہلا T20: آسٹریلیا نے پاکستان کو 29 رنز سے ہرادیا\n",
      "کرکٹ کے بعد بھارت کا کبڈی ٹیم بھی پاکستان بھیجنے سے انکار\n",
      "آئی سی سی کے براڈ کاسٹنگ رائٹس خریدنے والی کمپنی کا صبر جواب دے گیا\n",
      "بھارت نے تیسرے ٹی ٹوئنٹی میں جنوبی افریقا کو11 رنز سے شکست دیدی\n",
      "ثانیہ مرزا دبئی میں کھیلوں کی سفیر مقرر\n",
      "پاک آسٹریلیا پہلا ٹی 20 آج برسبین میں ہوگا\n",
      "کرکٹ ایک نازک دوراہے پر ہے،  شاہد آفریدی\n",
      "سہ فریقی انڈر 19، پاکستان کا کامیاب آغاز\n",
      "پی ایچ ایف نے تین رکنی نئی قومی سلیکشن کمیٹی تشکیل دے دی\n",
      "ایشین کرکٹ کونسل نے پہلے انڈر 19 ویمن ایشیا کپ کا شیڈول جاری کردیا\n",
      "چیمپئنز ٹرافی جنوبی افریقہ منتقل کرنے کا بھارتی شوشہ، آئی سی سی اور افریقی بورڈ لاعلم\n",
      "پاکستان آسٹریلیا ٹی ٹوئنٹی میچ بارش سے متاثر ہونے کا خدشہ\n",
      "چیمپئنز ٹرافی: پاکستان ہائبرڈ ماڈل قبول نہ کرنے کے مؤقف پر سختی سے قائم\n",
      "شاہین آفریدی اور بابر اعظم کی ون ڈے رینکنگ میں ’ٹاپ‘ پوزیشن\n",
      "بابر اعظم نے عثمان خواجہ کی فاؤنڈیشن کو ٹیسٹ جرسی عطیہ کر دی\n",
      "آسٹریلیا میں آسٹریلین کنڈیشن میں کھیلنا آسان نہیں: محمد رضوان\n",
      "پی سی بی کے آئی سی سی سے پوچھے گئے سوالات سامنے آگئے\n",
      "چیمپئنز ٹرافی معاملہ پر پاکستان کا موقف درست ہے، خالد محمود\n",
      "تبریز شمسی، کارلوس براتھ ویٹ کا لاہور قلندرز سے معاہدہ\n",
      "پاکستان اسپورٹس بورڈ نے پی ایف ایف کو مینڈیٹ سے تجاوز پر خط لکھ دیا\n",
      "بھارتی ہٹ دھرمی، پاکستان کے واضح موقف نے آئی سی سی کی پریشانی بڑھادی\n",
      "ٹی20 بلائنڈ ورلڈ کپ کیلئے پاکستان کی 16 رکنی ٹیم کا اعلان\n",
      "ٹی ٹوئنٹی سیریز میں بھی آسٹریلیا کو واش آؤٹ کریں گے: محمد رضوان\n",
      "چیمپئنز ٹرافی: بھارت کے انکار پر پی سی بی کا آئی سی سی کو خط\n",
      "بھارتی ٹیم آئے یا نہ آئے، بلائنڈ ورلڈ کپ کی میزبانی پاکستان ہی کریگا: سید سلطان شاہ\n",
      "نیوزی لینڈ کی آل راؤنڈر میلی کر آئی سی سی پلیئر آف دی منتھ قرار\n",
      "نعمان علی اکتوبر کے آئی سی سی پلیئر آف دی منتھ قرار\n",
      "پاکستان سے شکست پر کرکٹ آسٹریلیا کے چیف ایگزیکٹیو نک ہوکلے مایوس\n",
      "بلائنڈ ٹی ٹوئنٹی ورلڈ کپ: بھارتی ٹیم کو پاکستان آنے کیلئے تاحال منظوری نہ مل سکی\n",
      "افغانستان نے بنگلادیش کو تیسرے میچ میں ہراکر سیریز اپنے نام کرلی\n",
      "پاکستانی کھلاڑی سے شرٹ ملنے پر بھارتی لڑکا خوشی سے نہال\n",
      "چیمپئنز ٹرافی: پاک بھارت تنازع، آئی سی سی بھی مالی نقصان کی زد میں\n",
      "پاکستان اور بھارت کا بیس بال سیریز ممکن بنانے پر اتفاق\n",
      "دنیا بدل رہی ہے، بھارتی حکومت بھی اپنا رویہ بدلے، راجا پرویز اشرف\n",
      "حارث رؤف نے آسٹریلوی بیٹنگ کو ایکسپوز کردیا، جیسن گلیسپی\n",
      "یہ دن کا خواب تھا بھارت چیمپئنز ٹرافی کیلئے پاکستان آئے گا: محمد حفیظ\n",
      "چیمپئنز ٹرافی 2025ء: بھارت کی جگہ کون لے گا؟\n",
      "چیمپئنز ٹرافی: پاکستان کے مائنس بھارت کی تجویز دینے کا امکان\n",
      "جوش فلپ T20 سیریز کیلئے اسکواڈ میں شامل\n",
      "پاکستان جانے یا نہ جانے کا فیصلہ ہمارے ہاتھ میں نہیں ہے، روہت شرما\n",
      "دبئی، پاکستان بیس بال یونائیٹڈ کلاسک کا چمپیئن بن گیا\n",
      "بھارت کے پاکستان آنے سے انکار کے بعد کیا کیا ہوسکتا ہے؟\n",
      "تھل جیپ ریلی: پری پیئرڈ کیٹیگری میں آصف فضل چوہدری کی پہلی پوزیشن\n",
      "آئی سی سی ایونٹ میں ہائیبرڈ ماڈل نہیں چل سکتا، راشد لطیف\n",
      "پاکستان، بھارت کو کھیلوں کی ثالثی عدالت میں لے جاسکتا ہے، بھارتی میڈیا کا دعویٰ\n",
      "پاکستانی فیلڈز نے کمال کر دکھایا، 8 کیچز پکڑے\n",
      "پاکستان کا کھیلوں کے ہر فورم پر بھارت کو چیلنج کرنے کا فیصلہ، ذرائع\n",
      "ون ڈے سیریز: حارث کی آسٹریلیا کیخلاف عمدہ بولنگ\n",
      "بھارت نہیں آئے گا تو پاکستان اسکے خلاف کبھی کوئی میچ نہیں کھیلے گا، حکومتی ذرائع\n",
      "آسٹریلیا کیخلاف پاکستان کی ون ڈے سیریز میں کامیابی پر سابق قومی کرکٹرز کی مبارکباد\n",
      "پاکستان سے سیریز میں ہم مکمل طور پر آؤٹ کلاس ہوئے: آسٹریلوی کپتان\n",
      "محمد رضوان نے آسٹریلیا کیخلاف جیت کا کریڈٹ کس کو دیا؟\n",
      "آسٹریلیا کو ہوم گراؤنڈ پر شکست دیکر کھلاڑیوں نے قوم کا سر فخر سے بلند کیا، محسن نقوی\n",
      "پاکستان نے آسٹریلیا کو ون ڈے سیریز ہرا دی\n",
      "چیمپئنز ٹرافی کے شیڈول کا اعلان تاخیر کا شکار\n",
      "اسنوکر کے ورلڈ چیمپئن محمد آصف وطن واپس پہنچ گئے\n",
      "پاکستان بمقابلہ آسٹریلیا، ODI سیریز کے فاتح کا فیصلہ آج ہوگا\n",
      "ماریہ خان کا سعودی لیگ میں فری کک پر گول\n",
      "بنگلا دیش نے افغانستان کو دوسرا ون ڈے ہرا دیا\n",
      "ورلڈ باڈی بلڈنگ چیمپئن شپ، پاکستان کے بلال احمد نے گولڈ میڈل جیت لیا\n",
      "کشمالہ طلعت 10 میٹر ایئر پسٹل میں ایشیا کی دوسری بہترین شوٹر قرار\n",
      "بیس بال یونائیٹڈ عرب کلاسک: پاکستان نے سیمی فائنل میں جگہ بنالی\n",
      "پاکستان کے پاس آسٹریلیا میں 22 سال بعد تاریخ بنانے کا موقع\n",
      "پاکستانی گالفر احمد بیگ کی بین الاقوامی مقابلوں میں نمایاں پرفارمنس کا سلسلہ جاری\n",
      "سید سمیر احمد نئے سی او او پی سی بی مقرر\n",
      "چیمپئنز ٹرافی: بھارت نے آئی سی سی کو پاکستان نہ جانے سے آگاہ کردیا، ہائبرڈ ماڈل کا امکان، رپورٹ\n",
      "پی سی بی بورڈ آف گورنرز کے اراکین کا قذافی اسٹیڈیم کا دورہ، ترقیاتی کاموں پر بریفنگ\n",
      "پاکستان کے طیب اکرام 8 سال کیلئے انٹرنیشنل ہاکی فیڈریشن کے صدر منتخب\n",
      "علیزا صابر فٹبال کے بعد کرکٹ میں بھی پاکستان کی نمائندگی کی خواہشمند\n",
      "پاک آسٹریلیا ون ڈے سیریز کا فیصلہ کُن میچ کل کھیلا جائے گا\n",
      "نیوزی لینڈ کے ہاتھوں وائٹ واش: بھارتی ٹیم کے تھنک ٹینک سے باز پرس\n",
      "ویمن فٹبال فرینڈلی میچ، پاکستان فٹبال فیڈریشن کو این او سی جاری نہ ہوسکا\n",
      "بیس بال: پاکستان نے بھارت کو آؤٹ کلاس شکست سے دوچار کردیا\n",
      "انٹرنیشنل ہاکی فیڈریشن کا رائزنگ اسٹار ایوارڈ سفیان خان کے نام\n",
      "پنجاب میں اسموگ، قائداعظم ٹرافی کے میچز کا شیڈول تبدیل\n",
      "ایڈیلیڈ میچ کے ہیرو حارث رؤف کی سالگرہ\n",
      "قذافی اسٹیڈیم اپ گریڈیشن پروجیکٹ تیزی سے تکمیل کی جانب بڑھنے لگا\n",
      "ویمنز کرکٹ ٹیم کی پرفارمنس میں بہتری آرہی ہے، ارم جاوید\n",
      "ندا ڈار قومی مینز ٹیم کی شاندار پرفارمنس پر خوش\n",
      "بھارتی بورڈ نے ٹیم کے نہ آنے سے آگاہ نہیں کیا، محسن نقوی\n",
      "امید ہے پاکستانی کرکٹ ٹیم فتح کا تسلسل برقرار رکھے گی: مریم نواز\n",
      "بھارت کا چیمپئنز ٹرافی کیلئے پاکستان آنے سے انکار\n",
      "آنیوالے دنوں میں پاکستان کرکٹ ٹیم کھویا ہوا مقام حاصل کر لے گی: عطاء تارڑ\n",
      "حارث رؤف نے ثقلین مشتاق اور شاہین آفریدی کا ریکارڈ توڑ دیا\n",
      "آسٹریلیا کیخلاف 6 کیچ، محمد رضوان نے ورلڈ ریکارڈ برابر کر دیا\n",
      "\t--> Successfully scraped 100 articles from 'sports'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 100 articles in 'world'.\n",
      "برطانیہ کے کچھ حصوں میں زرد موسم کی وارننگ جاری\n",
      "بیروت: اسرائیلی فوج کا بیروت میں رہائشی عمارت پر حملہ، 3 جاں بحق 9 زخمی\n",
      "غزہ: 24 گھنٹوں میں 24 فلسطینی شہید، 112 زخمی\n",
      "آلودہ ترین شہروں میں آج نئی دہلی کا پہلا نمبر\n",
      "سری لنکن انتخابات: صدر ڈسانائیکے 141 نشستوں کیساتھ کامیاب\n",
      "کھیل ملکوں کو جوڑنے کا ذریعہ ہوتے ہیں، امریکا\n",
      "ٹرمپ کے مشیر ایلون مسک کی اقوام متحدہ میں ایران کے مستقل مندوب سے ملاقات\n",
      "ٹرمپ کی جانب سے کابینہ کیلئے متنازع ناموں کے اعلان کا سلسلہ جاری\n",
      "آئندہ سوشل میڈیا سائٹ ایکس پر اپنا ادارتی مواد پوسٹ نہیں کرینگے، دی گارڈین\n",
      "ویزا ایمنسٹی اسکیم سے فوری فائدہ اُٹھائیں، دبئی حکام کی غیر قانونی تارکین وطن کو وارننگ\n",
      "خان یونس: اسرائیلی بمباری سے تباہ اسکول میں فلسطینی بچی کے پڑھنے کی ویڈیو وائرل\n",
      "اسرائیلی طیاروں کی بیروت پر بمباری جاری، چند گھنٹوں میں متعدد فضائی حملے\n",
      "ایران کا حجاب سے منحرف خواتین کیلئے کلینک کھولنے کا اعلان\n",
      "اسرائیلی فوج کے ہاتھوں 1 ماہ میں 20 امدادی کارکن قتل\n",
      "اسرائیل کا جنوبی لبنان میں اپنے 6 فوجیوں کی ہلاکت کا اعتراف\n",
      "روسی صدر پیوٹن پر تنقید کرنے واالا شیف سربیا کے ہوٹل میں مردہ پایا گیا\n",
      "یورپی یونین خارجہ پالیسی چیف کا اسرائیل سے سیاسی تعلقات ختم کرنے پر غور\n",
      "وائٹ ہاؤس، بائیڈن کی ٹرمپ سے 2 گھنٹے کی ملاقات\n",
      "بھارتی وزیراعلیٰ بھگونت مان کا مریم نواز کی اسموگ ڈپلومیسی پر جواب آگیا\n",
      "خفیہ دستاویزات لیک کرنے پر سی آئی اے اہلکار کمبوڈیا میں زیرحراست\n",
      "برطانیہ: سارہ قتل کیس: والد عرفان شریف نے قتل کی ذمہ داری قبول کرلی\n",
      "سعودی عرب کی اسرائیل کی جانب سے فلسطینوں کی نسل کشی کی مذمت\n",
      "ڈونلڈ ٹرمپ حکومت میں ایلون مسک بھی اہم عہدے پر تعینات ہونگے\n",
      "غزہ: اقوامِ متحدہ کی امداد کی تقسیم کے بعد اسرائیلی فورسز کا حملہ\n",
      "یوکرین: کیف میں دھماکے سنے گئے\n",
      "سعودی وزیرِ خارجہ شہزادہ فیصل بن فرحان بھارت پہنچ گئے\n",
      "غزہ میں اسرائیل کیساتھ امریکا بھی نسل کشی میں ملوث ہے: حماس\n",
      "اسرائیل نے غزہ میں انسانی امداد سے متعلق قانون کی خلاف ورزی نہیں کی، امریکا\n",
      "پینٹاگون کی خفیہ دستاویزات لیک، اہلکار کو 15 سال قید کی سزا\n",
      "امریکا، جان ریٹکلف سی آئی اے کے ڈائریکٹر منتخب\n",
      "کمسنوں پر جنسی تشدد  سامنے لانے میں ناکام چرچ آف انگلینڈ کے جسٹن ویلبی مستعفی\n",
      "حزب اللّٰہ کے شمالی اسرائیل میں راکٹ حملے، 2 اسرائیلی ہلاک\n",
      "تھائی پولیس نے اسرائیلی سیاحوں پر حملوں کا خدشہ ظاہر کر دیا، اسرائیلی سیکیورٹی ذرائع\n",
      "اسرائیلی فوج کا شمالی غزہ میں اپنے 4 فوجیوں کی ہلاکت کا اعتراف\n",
      "چین میں تیز رفتار گاڑی نے 35 افراد کو کچل کر ہلاک دیا\n",
      "اسرائیلی حکومت نے مغربی کنارے کو اسرائیل میں ضم کرنے کا ارادہ ظاہر کردیا\n",
      "صبح سے جاری اسرائیلی حملوں میں ابتک 21 فلسطینی شہید\n",
      "اسرائیلی حملوں کے پیشِ نظر ایران کی ’دفاعی سرنگ‘ بنانے کی تیاری\n",
      "امریکا: کرسٹی نوم سیکریٹری ہوم لینڈ سیکیورٹی منتخب\n",
      "شمالی کوریا اور  روس کے مابین تاریخی دفاعی معاہدے کی توثیق\n",
      "موسمیاتی فنانس پر دنیا کو ادائیگی کرنا ہو گی ورنہ انسانیت قیمت ادا کریگی: سیکریٹری جنرل یو این\n",
      "چیف پراسیکیوٹر پر جنسی ہراسانی کے الزامات، عالمی عدالتِ جرائم کا بیرونی تحقیقات کا اعلان\n",
      "متحدہ عرب امارات میں بارش اور سیلاب کا خطرہ بڑھ گیا\n",
      "مارکو روبیو کے وزیر خارجہ بنائے جانے کا امکان\n",
      "کعبۃ اللّٰہ کی حدود حطیم میں نوافل ادائیگی کیلئے اوقات مقرر\n",
      "فلسطین، لبنان اور ایران پر اسرائیلی حملوں کیخلاف عرب اسلامی سربراہ اجلاس کا اعلامیہ\n",
      "غزہ: القسام بریگیڈ کے حملے میں میجر سمیت  4 اسرائیلی فوجی ہلاک ہوگئے\n",
      "بابا صدیق کے قاتل کے والد نے بیٹے کے جرائم سے متعلق پردہ فاش کردیا\n",
      "شمالی غزہ میں بھوک و افلاس، امریکا کا اسرائیل سے ایک بار پھر اظہار تشویش\n",
      "عالمی برادری اسرائیل کو جنگ بندی پر مجبور کرے، محمد بن سلمان\n",
      "بھارتی سپریم کورٹ کا دہلی حکومت کو 25 نومبر تک پٹاخوں پر مستقل پابندی کا حکم\n",
      "سابق بھارتی کرکٹر سنجے بانگر کے بیٹے نے جنس تبدیل کروالی\n",
      "بابا صدیق کے قتل میں ملوث مرکزی ملزم گرفتار\n",
      "ماحولیاتی تبدیلی سے نمٹنے کیلئے ’کوپ 29‘ کا آغاز\n",
      "روس اور شمالی کوریا کی یوکرین سے ’کرسک‘ واپس لینے کی تیاری\n",
      "بھارت: جسٹس سنجیو کھنہ نے چیف جسٹس کا حلف اٹھا لیا\n",
      "بھارتی پیرا ملٹری فورس کے اہلکار نے خودکشی کر لی\n",
      "غزہ، لبنان، شام میں اسرائیلی حملوں میں درجنوں افراد شہید\n",
      "ریاض، عرب اسلامی سربراہی اجلاس آج ہوگا\n",
      "ٹرمپ کی فتح میں ایلون مسک کا کردار رہا، سوشل میڈیا پر دعویٰ\n",
      "حملے کا خوف، نیتن یاہو نے اپنا دفتر تہہ خانے میں منتقل کردیا\n",
      "سعودی آرمی چیف کا دورہ ایران، ہم منصب سے ملاقات، دفاعی سفارتکاری و دیگر امور پر گفتگو\n",
      "مقبوضہ کشمیر میں بھارتی فوج کی دہشتگردی جاری، سوپور میں 3 کشمیری شہید\n",
      "نیتن یاہو کا لبنان میں پیجر ڈیوائس حملوں اور حسن نصراللّٰہ کو شہید کرنے کا اعتراف\n",
      "ٹرمپ کی حلف برداری میں 71 دن باقی، دھماکہ خیز فیصلوں کا اعلان متوقع\n",
      "امریکی صدارتی انتخابات: ڈونلڈ ٹرمپ ساتوں سوئنگ اسٹیٹس میں کامیاب\n",
      "ڈھاکا: بی این پی کا حسینہ واجد کے ٹرائل اور نئے انتخابات کا مطالبہ\n",
      "اسرائیلی حملے، مزید 44 فلسطینی، 52 لبنانی شہید\n",
      "ٹرمپ کے مقابلے میں کملا ہیرس کو زیادہ ملنے والے یہودی ووٹ بھی نہ جتا سکے\n",
      "بائیڈن کا عہدہ چھوڑنے سے قبل یوکرین کو اربوں ڈالرز کی امداد دینے کا منصوبہ\n",
      "بابا صدیق کے قاتلوں سے کیا دینے کے وعدے کیے گئے؟\n",
      "ڈونلڈ ٹرمپ کے قتل کی سازش میں ایران ملوث ہے: امریکا\n",
      "فلسطینی صدر کا ٹرمپ کو فون، کامیابی پر مبارک باد دی\n",
      "امریکا کا قطر سے حماس وفد کو ملک بدر کرنے کا مطالبہ\n",
      "غزہ: معصوم بچی ٹھنڈ سے بچنے کیلئے فوڈ کینز کو جوتوں کے طور پر استعمال کرنے پر مجبور\n",
      "امریکی انتخابات: نیواڈا سے منتخب رکن اسمبلی حنادی ندیم کون؟\n",
      "غزہ میں اسرائیلی فورسز کی فلسطینیوں کی نسل کشی پر اقوام متحدہ کے انکشافات\n",
      "اسرائیلی وزیراعظم نیتن یاہو نے خفیہ ایجنسی موساد کو نئی ہدایات جاری کردیں\n",
      "اسموگ پر قابو پانے کیلئے بڑی تعداد میں فوارے نصب کیے جائیں، فرانسیسی ماہر موسمیات\n",
      "امید ہے ٹرمپ اپنے دوسرے دور صدارت میں جنگیں رکوائیں گے، ترک صدر\n",
      "اسرائیلی وزیراعظم کے دفتر پر نیتن یاہو کے قریبی ساتھیوں کی  حساس ویڈیوز جمع کرنے کا الزام\n",
      "‎امریکی مسلمانوں، عرب ووٹرز نے ڈیموکریٹس کو چھوڑ دیا: سی اے آئی آر\n",
      "ایمسٹرڈیم: اسرائیلی فٹبال شائقین پر حملہ، 11 زخمی\n",
      "ڈونلڈ ٹرمپ نے سوزی وائلز کو وائٹ ہاؤس کی چیف آف اسٹاف مقرر کر دیا\n",
      "ڈونلڈ ٹرمپ کا روسی صدر پیوٹن سے بات کی خواہش کا اظہار\n",
      "روسی صدر پیوٹن کی ٹرمپ کو انتخاب میں کامیابی پر مبارکباد\n",
      "دنیا کی کوئی طاقت آرٹیکل 370 دوبارہ نہیں لاسکتی، نریندر مودی\n",
      "ٹرمپ کی کامیابی کے بعد خارجہ پالیسی پر انتہاپسندوں کے غلبے کا خدشہ ہے، امریکی جریدہ\n",
      "اسرائیل نے امریکا سے 25 جنگی طیارے خریدنے کا معاہدہ کرلیا\n",
      "وزیر خزانہ کی برطرفی سے جرمن حکومت کو خطرہ لاحق، اپوزیشن کا اعتماد کا ووٹ لینے کا مطالبہ\n",
      "فتح کے بعد فیملی تصویر، ٹرمپ کی اہلیہ غائب، ایلون مسک اہل خانہ کے ساتھ موجود\n",
      "یوگنڈا کی پارلیمنٹ ریسلنگ رنگ میں بدل گئی، ارکان ایک دوسرے پر چڑھ دوڑے\n",
      "حزب اللّٰہ راکٹ حملہ: اسرائیل کی 1 فوجی کی ہلاکت، 3 زخمی ہونے کی تصدیق\n",
      "ڈونلڈ ٹرمپ کی کامیابی سے ایلون مسک کو راتوں رات اربوں ڈالرز کا فائدہ\n",
      "آسٹریلیا: 16 سال سے چھوٹے بچوں کے سوشل میڈیا کے استعمال پر پابندی کی تیاری\n",
      "بارک اوباما نے ڈونلڈ ٹرمپ کے کامیابی پر کیا کہا؟\n",
      "الیکشن میں ہار مانتی ہوں: کملا ہیرس\n",
      "مقبوضہ کشمیر اسمبلی میں خصوصی حیثیت کی بحالی کی قرارداد منظور\n",
      "\t--> Successfully scraped 98 articles from 'world'.\n",
      "Scraping category 'health-science'...\n",
      "\t--> Found 101 articles in 'health-science'.\n",
      "پنجاب میں اسموگ کے باعث 19 لاکھ سے زائد افراد اسپتال پہنچ گئے\n",
      "تھوڑا آرام اپنے پیروں اور ایڑیوں کو بھی دیں\n",
      "ڈیمینشیا کی علامات برسوں پہلے ظاہر ہونا شروع ہوجاتی ہیں، تحقیق\n",
      "بلوچستان کے ضلع جعفر آباد سے پولیو کیس رپورٹ، تعداد 49 ہوگی\n",
      "سورج کی روشنی، دودھ اور مچھلی کے استعمال سے وٹامن ڈی کی کمی پوری ہوسکتی ہے، ماہرین\n",
      "بلوچستان میں پولیو کا ایک اور کیس رپورٹ\n",
      "سینیٹر فوزیہ ارشد کی ڈریپ ایکٹ میں ترمیم کی حمایت\n",
      "ملتان: نشتر اسپتال میں مریضوں میں ایڈز پھیلنے کی وجہ سامنے آ گئی\n",
      "پاکستان سمیت آج دنیا بھر میں ذیابیطس کا عالمی دن منایا جارہا ہے\n",
      "148 دن میں ساڑھے 53 لاکھ مریضوں کا انکی دہلیز پر علاج کیا گیا: مریم نواز\n",
      "ملتان: نشتر اسپتال میں 30 مریض ایڈز میں مبتلا\n",
      "گلے میں خراش یا سوزش ہے تو چند آزمودہ نسخے آپ کیلئے\n",
      "کراچی: مبینہ غلط انجکشن سے 3 سالہ بچی کا انتقال\n",
      "اسموگ سے بچوں کے پھپھڑے اور دماغی نشوونما متاثر ہو رہے ہیں: عالمی ادارۂ اطفال کا انتباہ\n",
      "بال خوبصورت، لمبے اور گھنے بنائیں گھر پر تیار شیمپو سے\n",
      "روزانہ کی خوراک سے صرف ایک گرام نمک کم کرنے کے فوائد\n",
      "رواں سال کراچی میں ڈینگی کے 1 ہزار 843 کیسز\n",
      "سندھ: رواں سال ڈینگی کے 2 ہزار 192 کیسز رپورٹ\n",
      "موسمبی سے صحت بھی، خوبصورتی بھی\n",
      "پنجاب میں ڈینگی کے 79 کیسز رپورٹ\n",
      "خوشی حاصل کرنے کی خواہش کا ڈوپامین سے کیا تعلق؟ اسے کنٹرول کیسے کریں؟\n",
      "لاہور میں اسموگ کی شدت برقرار، بیماریاں پھیلنے لگیں\n",
      "پاکستان میں 48ویں پولیو وائرس کیس کی تصدیق\n",
      "انسانوں کی عمر کتنی طویل ہو سکتی ہے؟ سائنسدانوں نے پتہ لگا لیا\n",
      "ایم ڈی کیٹ کے ہی مخالف ہوں: ڈاکٹر عذرا فضل پیچوہو\n",
      "قوتِ برداشت میں کمی محسوس ہو تو کیا کریں؟\n",
      "کونسی غذائیں ڈپریشن کم کرتی ہیں؟\n",
      "کوئٹہ: ادویات چھپانے پر 3 فارماسسٹس گرفتار، 2 کی تلاش جاری\n",
      "پنجاب: اسموگ سے آنکھ، ناک، کان، گلے کی بیماریاں پھیلنے لگیں\n",
      "ملک میں ایک اور پولیو کیس کی تصدیق، تعداد 46 ہو گئی\n",
      "بلوچستان: وزیرِ اعلیٰ کا انسدادِ پولیو کیلئے 10 روز میں جامع روڈ میپ مرتب کرنے کا حکم\n",
      "بلوچستان میں انسدادِ پولیو مہم اختتام پزیر، 99 فیصد ہدف حاصل\n",
      "پنجاب: ڈینگی کے مزید 118 کیسز رپورٹ\n",
      "خناق کیا ہے، بچاؤ کیلئے ویکسینیشن کیوں ضروری؟\n",
      "دادو: 7 سالہ بچی میں پولیو وائرس کا شبہ، کراچی کے اسپتال منتقل\n",
      "پنجاب بھر میں ڈینگی کیسز کے اعداد و شمار جاری\n",
      "51 فیصد افراد کے اینٹی بائیوٹک ادویات از خود استعمال کرنے کا انکشاف\n",
      "پنجاب میں ڈینگی وائرس کے مزید 141 کیسز رپورٹ\n",
      "کراچی: اکتوبر میں ڈینگی کے 363 کیسز رپورٹ\n",
      "ورزش سے قبل درست غذاؤں کا انتخاب کیوں ضروری؟\n",
      "ملک میں پولیو وائرس کے مزید دو کیسز سامنے آگئے\n",
      "بینائی کی حفاظت آپ کے ہاتھوں میں، اس کا خیال رکھیں\n",
      "گزشتہ برس دنیا بھر میں تپ دق کے 82 لاکھ نئے کیسز کی تشخیص ہوئی، عالمی ادارہ صحت\n",
      "اسلام آباد میں چکن گونیا کا پہلا تصدیق شدہ کیس رپورٹ\n",
      "پاکستان میں تپِ دق کے کیسز میں نمایاں اضافہ\n",
      "وزن کم کرنے کیلئے ورزش سے قبل کیا کھائیں اور کیا نہیں؟\n",
      "اورکزئی: حملے کے بعد رکی انسدادِ پولیو مہم کا آج سے دوبارہ آغاز\n",
      "بلوچستان: رواں سال پولیو کے 21 کیسز رپورٹ ہوئے\n",
      "ڈینگی پھیلانے والا مچھر ہی چکن گونیا کا باعث بنتا ہے، ماہرین\n",
      "گورنر سندھ کی ملیریا کے کیسز میں اضافے پر ایکشن کی ہدایت\n",
      "بلوچستان میں انسدادِ پولیو مہم جاری\n",
      "سندھ بھر میں ملیریا کے کیسز میں مسلسل اضافہ\n",
      "موبائل کا غلط انداز میں استعمال ذہنی مسائل کا سبب بن سکتا ہے، تحقیق\n",
      "پنجاب حکومت کا 150 بنیادی مراکز صحت آؤٹ سورس کرنے کافیصلہ\n",
      "کے پی کے: ڈینگی کے مزید 56 کیسز رپورٹ\n",
      "پولیو قطرے پلانے سے انکار کرنیوالے 50 فیصد لوگوں کی نشاندہی کر لی: مراد علی شاہ\n",
      "کے پی کے: 33 اضلاع میں 5 روزہ انسدادِ پولیو مہم کا پہلا مرحلہ شروع\n",
      "ایک سیب روزانہ، واقعی رکھے ڈاکٹر سے دور؟\n",
      "سندھ: 7 روزہ انسدادِ پولیو مہم کا آغاز آج ہو گا\n",
      "سندھ بھر میں سات روزہ انسداد پولیو مہم کا آغاز کل سے ہوگا، ایمرجنسی آپریشن سینٹر\n",
      "لاہور میں آلودگی سے بیماریاں پھیلنے لگیں\n",
      "پولیو سے نمٹنے کیلئے پاکستان و دیگر ممالک میں سعودی منصوبوں پر عملدرآمد\n",
      "’سُپر فوڈ‘ پالک, کئی موذی امراض کے خلاف مؤثر\n",
      "کراچی: شارجہ سے آنے والی پرواز کے مسافر میں منکی پاکس کی علامات\n",
      "پاکستان میں ایک اور بچہ پولیو سے معذور ہوگیا\n",
      "کراچی میں ڈینگی کے کیسز میں اضافہ، آج شہر میں ڈینگی کے 18 کیسز رپورٹ\n",
      "کراچی: رواں سال ملیریا کے 1 ہزار 935، چکن گونیا کے 172 کیسز رپورٹ\n",
      "کراچی: رواں سال ڈینگی کے 1647 کیسز سامنے آ گئے\n",
      "پنجاب: ڈینگی کے مزید 128 کیسز رپورٹ، 1 مریض کا انتقال\n",
      "چکن گونیا سے بچاؤ کیسے ممکن؟\n",
      "لاہور: فضائی آلودگی بڑھ گئی، بیماریاں پھیلنے کا خطرہ، شہری کیا احتیاطی تدابیر اختیار کریں؟\n",
      "علماء پولیو سے متعلق آگاہی پھیلانے میں کردار ادا کریں: علی امین گنڈاپور\n",
      "پاکستان میں پہلی پولیو ویکسین آصفہ بھٹو کو پلائی گئی: بلاول بھٹو\n",
      "ملک میں پولیو وائرس کا 40 واں کیس رپورٹ\n",
      "لاہور کی فضا میں آلودگی مزید بڑھ گئی\n",
      "خوبصورت اور صحت مند بال اب خواب نہیں\n",
      "مٹھی میں مختلف بیماریوں کے باعث 24 گھنٹوں میں 6 بچے جاں بحق\n",
      "کراچی: نومبر میں ڈینگی کیسز کم ہونے کا امکان\n",
      "چھاتی کے سرطان کے سالانہ 38000 کیسز رپورٹ ہوتے ہیں: ترجمان سندھ حکومت\n",
      "بلوچستان میں رواں سال سب سے زیادہ 20 پولیو کیسز رپورٹ\n",
      "بریسٹ کینسر کیوں ہوتا ہے؟ بچاؤ کیسے ممکن؟\n",
      "عالمی ادارہ صحت نے مصر کو ملیریا سے پاک قرار دے دیا\n",
      "کراچی: 19 اکتوبر کو ڈینگی کے 10 کیسز رپورٹ ہوئے\n",
      "ملک میں رواں سال پولیو سے 3 بچے جاں بحق\n",
      "سندھ میں پولیو وائرس کے مزید 2 کیسز کی تصدیق، ملک بھر میں تعداد 39 ہوگئی\n",
      "پنجاب کو مکمل پولیو فری بنانا ہمارا عزم ہے: وزیرِ اعلیٰ پنجاب\n",
      "کراچی: بچوں میں نمونیا کیسز میں اضافہ\n",
      "ہر 9 میں سے 1 خاتون کو چھاتی کے سرطان کا خطرہ ہے: صدر زرداری\n",
      "روزانہ آدھا چمچ زیتون کے تیل کا استعمال الزائمر کے خطرے کو کم کرتا ہے، تحقیق\n",
      "پنجاب: ڈینگی کے مزید 112 کیسز رپورٹ\n",
      "سندھ میں جان بچانے والی ادویات کی شدید قلت\n",
      "کراچی: 3 روزہ ہیلتھ ایشیاء ایکسپو کا افتتاح\n",
      "سندھ انفیکشس ڈیزیز اسپتال کے عملے کے خناق سے متاثر ہونے کی خبر بے بنیاد قرار\n",
      "طبی عملے کو خناق کی ویکسین کا بوسٹر لگوانے کا فیصلہ\n",
      "دنیا بھر میں شدید غذائی قلت کا شکار 20 لاکھ بچوں کو موت کا خطرہ\n",
      "سندھ انفیکشس ڈیزیز اسپتال کے طبی عملے کو خناق کا بوسٹر لگوانے کا فیصلہ\n",
      "کافی نوش کیجیے، یادداشت بہتر بنائیے!\n",
      "خوبصورت نظر آنا ہے تو چند عادتیں چھوڑ دیں\n",
      "سندھ: جنوری تا اکتوبر رپورٹ ہوئے ملیریا کیسز کی تعداد سامنے آ گئی\n",
      "\t--> Successfully scraped 99 articles from 'health-science'.\n"
     ]
    }
   ],
   "source": [
    "express_df = scraper.get_express_articles()\n",
    "dunya_df = scraper.get_dunya_articles()\n",
    "# geo_df = scraper.get_geo_articles()\n",
    "jang_df = scraper.get_jang_articles()\n",
    "# dawn_df = scraper.get_dawn_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 5 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat([express_df, dunya_df, jang_df], ignore_index=True)\n",
    "combined_df.to_csv('data3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
