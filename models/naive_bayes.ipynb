{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model 1: MultiNomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2185 entries, 0 to 2184\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          2185 non-null   int64 \n",
      " 1   title       2185 non-null   object\n",
      " 2   link        2185 non-null   object\n",
      " 3   content     2185 non-null   object\n",
      " 4   gold_label  2185 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 85.5+ KB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ادارہ 'علم دوست' کی جانب سے'معلوماتِ عامہ' کے ...</td>\n",
       "      <td>https://www.express.pk/story/2733338/idara-ilm...</td>\n",
       "      <td>ادارہ 'علم دوست' کی جانب سے'معلوماتِ عامہ' کے ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>فلم ساز ودھو ونود چوپڑا کی نئی فلم ’زیرو سے ری...</td>\n",
       "      <td>https://www.express.pk/story/2733336/director-...</td>\n",
       "      <td>معروف فلم ساز ودھو ونود چوپڑا نے اپنی نئی فلم ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>عمر بڑھنے کے ساتھ وزن کم کرنا مشکل ہوتا ہے، اب...</td>\n",
       "      <td>https://www.express.pk/story/2733331/umer-barh...</td>\n",
       "      <td>ابھیشیک بچن نے اپنی نئی فلم ’آئی وانٹ ٹو ٹاک‘ ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ملائکہ اروڑا والد کے انتقال کے بعد کام پر واپس...</td>\n",
       "      <td>https://www.express.pk/story/2733327/malaikaar...</td>\n",
       "      <td>مشہور اداکارہ ملائکہ اروڑا حال ہی میں والد کے ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ڈائریکٹر دھرمیش درشن کا دو بار شاہ رخ خان کی ف...</td>\n",
       "      <td>https://www.express.pk/story/2733325/directord...</td>\n",
       "      <td>بالی ووڈ کے معروف ہدایتکار دھرمیش درشن نے حالی...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  ادارہ 'علم دوست' کی جانب سے'معلوماتِ عامہ' کے ...   \n",
       "1   1  فلم ساز ودھو ونود چوپڑا کی نئی فلم ’زیرو سے ری...   \n",
       "2   2  عمر بڑھنے کے ساتھ وزن کم کرنا مشکل ہوتا ہے، اب...   \n",
       "3   3  ملائکہ اروڑا والد کے انتقال کے بعد کام پر واپس...   \n",
       "4   4  ڈائریکٹر دھرمیش درشن کا دو بار شاہ رخ خان کی ف...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.express.pk/story/2733338/idara-ilm...   \n",
       "1  https://www.express.pk/story/2733336/director-...   \n",
       "2  https://www.express.pk/story/2733331/umer-barh...   \n",
       "3  https://www.express.pk/story/2733327/malaikaar...   \n",
       "4  https://www.express.pk/story/2733325/directord...   \n",
       "\n",
       "                                             content     gold_label  \n",
       "0  ادارہ 'علم دوست' کی جانب سے'معلوماتِ عامہ' کے ...  Entertainment  \n",
       "1  معروف فلم ساز ودھو ونود چوپڑا نے اپنی نئی فلم ...  Entertainment  \n",
       "2  ابھیشیک بچن نے اپنی نئی فلم ’آئی وانٹ ٹو ٹاک‘ ...  Entertainment  \n",
       "3  مشہور اداکارہ ملائکہ اروڑا حال ہی میں والد کے ...  Entertainment  \n",
       "4  بالی ووڈ کے معروف ہدایتکار دھرمیش درشن نے حالی...  Entertainment  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/tahafaisal/Desktop/ml-news-classification/data/FINAL_DATASET.csv')\n",
    "\n",
    "data.info()\n",
    "print()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    return re.sub(r'[٠١٢٣٤٥٦٧٨٩]', '', text)  # Remove Urdu numerals\n",
    "\n",
    "def normalize_urdu(text):\n",
    "    text = re.sub(r'[؁؂؃؄؅؆؇؈؉؊؋،؛؟]', '', text)  # Remove Urdu punctuation\n",
    "    text = re.sub(r'[آإأٱ]', 'ا', text)  # Normalize different forms of 'alif'\n",
    "    text = re.sub(r'[ىېۍ]', 'ی', text)  # Normalize different forms of 'ye'\n",
    "    text = re.sub(r'[ۀہ]', 'ہ', text)  # Normalize 'heh'\n",
    "    text = re.sub(r'[ؤو]', 'و', text)  # Normalize 'waw'\n",
    "    text = re.sub(r'[ءئ]', 'ی', text)  # Normalize 'hamza' with 'ye'\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return re.findall(r'\\w+', text)  # Extract words using regex\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "def lemmatize_custom(word):\n",
    "    if word.endswith('نا') or word.endswith('تے'):\n",
    "        return word[:-2]  # Strip suffix\n",
    "    elif word.endswith('ا') or word.endswith('ی'):\n",
    "        return word[:-1]  # Remove singular/plural suffix\n",
    "    return word\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    return [lemmatize_custom(word) for word in tokens]\n",
    "\n",
    "def remove_non_informative_words(tokens):\n",
    "    return [word for word in tokens if len(word) > 1]  # Remove single-character tokens\n",
    "\n",
    "\n",
    "def preprocess_urdu_text(text):\n",
    "    text = clean_text(text)  # Clean text\n",
    "    text = normalize_unicode(text)  # Normalize Unicode\n",
    "    text = normalize_urdu(text)  # Normalize Urdu-specific characters\n",
    "    tokens = tokenize_text(text)  # Tokenize text\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    tokens = remove_non_informative_words(tokens)# Apply lemmatization\n",
    "    return ' '.join(tokens)  # Return preprocessed text\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "data['title'] = data['title'].apply(preprocess_urdu_text)\n",
    "data['content'] = data['content'].apply(preprocess_urdu_text)\n",
    "data['combined'] = data['title'] + \" \" + data['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748,) (1748,)\n",
      "(437,) (437,)\n"
     ]
    }
   ],
   "source": [
    "X = data['content'] \n",
    "y = data['gold_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Multinomial Naive Bayes From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}  \n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, corpus):\n",
    "\n",
    "        unique_words = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split() \n",
    "            unique_words.update(words)  \n",
    "\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(unique_words))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "\n",
    "        vector = [0] * self.vocab_size\n",
    "        \n",
    "        words = sentence.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                index = self.vocabulary[word]\n",
    "                vector[index] += 1\n",
    "\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords()\n",
    "model.fit(X_train)\n",
    "\n",
    "vector_training = [model.vectorize(doc) for doc in X_train]\n",
    "vector_test = [model.vectorize(doc) for doc in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classprobs = {}  \n",
    "        self.wordprobs = {}  \n",
    "        self.vocab_size = 0    \n",
    "        self.bow = BagOfWords()  # Assume BagOfWords handles vectorization without LabelEncoder                \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.bow.fit(X)         \n",
    "        X_vectorized = [self.bow.vectorize(doc) for doc in X] \n",
    "        self.vocab_size = self.bow.vocab_size \n",
    "        n_docs = len(X)\n",
    "        \n",
    "        # Get unique classes as they are\n",
    "        unique_classes = np.unique(y)  \n",
    "        class_counts = {c: 0 for c in unique_classes}\n",
    "        word_counts = {c: np.zeros(self.vocab_size) for c in unique_classes}\n",
    "\n",
    "        for i in range(n_docs):\n",
    "            c = y.iloc[i]  # Updated to y.iloc[i] to handle pandas Series correctly\n",
    "            class_counts[c] += 1\n",
    "            word_counts[c] += X_vectorized[i]\n",
    "\n",
    "        for c in unique_classes:\n",
    "            self.classprobs[c] = class_counts[c] / n_docs\n",
    "            total_words_in_class = np.sum(word_counts[c])\n",
    "            self.wordprobs[c] = (word_counts[c] + 1) / (total_words_in_class + self.vocab_size)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_vectorized = np.array([self.bow.vectorize(doc) for doc in X])\n",
    "        predictions = []\n",
    "        \n",
    "        for doc_vec in X_vectorized:\n",
    "            class_scores = {}\n",
    "            for c in self.classprobs:\n",
    "                log_prob_c = np.log(self.classprobs[c])\n",
    "                log_prob_x_given_c = np.sum(doc_vec * np.log(self.wordprobs[c]) + (1 - doc_vec) * np.log(1 - self.wordprobs[c]))\n",
    "                class_scores[c] = log_prob_c + log_prob_x_given_c\n",
    "            \n",
    "            best_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(best_class)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually Implemented Naive Bayes:\n",
      "Accuracy:  0.9862700228832952\n",
      "Precision:  0.9857011494252873\n",
      "Recall:  0.9850844561770152\n",
      "F1 Score:  0.9853792307812949\n",
      "[[ 74   0   0   2   0]\n",
      " [  0  86   1   0   0]\n",
      " [  0   1  98   0   0]\n",
      " [  1   0   1  73   0]\n",
      " [  0   0   0   0 100]]\n"
     ]
    }
   ],
   "source": [
    "model = MultiNaiveBayes()\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "confusionmatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Manually Implemented Naive Bayes:\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(confusionmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sklearn MultiNomial Naive Bayes:\n",
      "Accuracy:  0.9862700228832952\n",
      "Precision:  0.9857011494252873\n",
      "Recall:  0.9850844561770152\n",
      "F1 Score:  0.9853792307812949\n",
      "[[ 74   0   0   2   0]\n",
      " [  0  86   1   0   0]\n",
      " [  0   1  98   0   0]\n",
      " [  1   0   1  73   0]\n",
      " [  0   0   0   0 100]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(vector_training, y_train)\n",
    "mnbprediction = model.predict(vector_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, mnbprediction)\n",
    "precision = precision_score(y_test, mnbprediction, average='macro')\n",
    "recall = recall_score(y_test, mnbprediction, average='macro')\n",
    "f1 = f1_score(y_test, mnbprediction, average='macro')\n",
    "confusionmatrix = confusion_matrix(y_test, mnbprediction)\n",
    "\n",
    "print(\"For Sklearn MultiNomial Naive Bayes:\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(confusionmatrix)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
